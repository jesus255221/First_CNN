{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.461 116.146 116.30417\n",
      "40.09 39.824 39.935085\n"
     ]
    }
   ],
   "source": [
    "Urban_station_lonlats = np.array([\n",
    "['dongsi_aq',116.417,39.929],\n",
    "['tiantan_aq',116.407,39.886],\n",
    "['guanyuan_aq',116.339,39.929],\n",
    "['wanshouxigong_aq',116.352,39.878],\n",
    "['aotizhongxin_aq',116.397,39.982],\n",
    "['nongzhanguan_aq',116.461,39.937],\n",
    "['wanliu_aq',116.287,39.987],\n",
    "['beibuxinqu_aq',116.174,40.09],\n",
    "['zhiwuyuan_aq',116.207,40.002],\n",
    "['fengtaihuayuan_aq',116.279,39.863],\n",
    "['yungang_aq',116.146,39.824],\n",
    "['gucheng_aq',116.184,39.914]\n",
    "])\n",
    "lons = np.array(Urban_station_lonlats[:,1],dtype = np.float32)\n",
    "lats = np.array(Urban_station_lonlats[:,2],dtype = np.float32)\n",
    "print(lons.max(),lons.min(),lons.mean())\n",
    "print(lats.max(),lats.min(),lats.mean())\n",
    "X_length = 93.6\n",
    "Y_length = 41.2\n",
    "#Change X Y to degree\n",
    "Y_degree = Y_length / 111\n",
    "#Consider the length of the latitude line\n",
    "X_degree = X_length / (40000 * np.cos(lats.mean() / 180 * 3.1415926) / 360)\n",
    "#X_degree = X_length / 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39.6 39.7 39.8 39.9 40.  40.1 40.2 40.3]\n",
      "[115.2 115.3 115.4 115.5 115.6 115.7 115.8 115.9 116.  116.1 116.2 116.3\n",
      " 116.4 116.5 116.6 116.7 116.8 116.9 117.  117.1 117.2 117.3]\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "#Initialize the step fo building \n",
    "lat_step = np.arange(lats.mean() - Y_degree,lats.mean() + Y_degree,.1)\n",
    "lon_step = np.arange(lons.mean() - X_degree,lons.mean() + X_degree,.1)\n",
    "#Build the step map\n",
    "for index in range(len(lat_step)):\n",
    "    lat_step[index] = round(lat_step[index],1)\n",
    "for index in range(len(lon_step)):\n",
    "    lon_step[index] = round(lon_step[index],1)\n",
    "print(lat_step)\n",
    "print(lon_step)\n",
    "#Get the size of point waiting to be analyzed\n",
    "point_size = len(lat_step) * len(lon_step)\n",
    "with open(\"lat_lon_range.pkl\",\"wb\") as file:\n",
    "    pickle.dump((lat_step,lon_step),file)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        stationName  longitude  latitude             utc_time  temperature  \\\n",
      "0  beijing_grid_000      115.0      39.0  2017-01-01 00:00:00        -5.47   \n",
      "1  beijing_grid_001      115.0      39.1  2017-01-01 00:00:00        -5.53   \n",
      "2  beijing_grid_002      115.0      39.2  2017-01-01 00:00:00        -5.70   \n",
      "3  beijing_grid_003      115.0      39.3  2017-01-01 00:00:00        -5.88   \n",
      "4  beijing_grid_004      115.0      39.4  2017-01-01 00:00:00        -5.34   \n",
      "\n",
      "   pressure  humidity  wind_direction  wind_speed/kph  \n",
      "0    984.73     76.60           53.71            3.53  \n",
      "1    979.33     75.40           43.59            3.11  \n",
      "2    963.14     71.80            0.97            2.75  \n",
      "3    946.94     68.20          327.65            3.84  \n",
      "4    928.80     58.81          317.85            6.14  \n"
     ]
    }
   ],
   "source": [
    "#Use panda to read csv\n",
    "beijing_weather_data = pd.read_csv(\"Beijing_historical_meo_grid.csv\")\n",
    "print(beijing_weather_data.head())\n",
    "#Get the value of beijing data\n",
    "beijing_weather_data_value = beijing_weather_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from datetime import datetime\n",
    "#Build the datetime object for date delta calculation\n",
    "for index in tqdm(range(len(beijing_weather_data_value))):\n",
    "    beijing_weather_data_value[index][3] = datetime.strptime(\n",
    "        beijing_weather_data_value[index][3],'%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Caculate the days between June 31 to March 1\n",
    "start = date(2017,3,1)\n",
    "end = date(2017,7,1)\n",
    "#Set the correct array size\n",
    "collection_size = point_size * (end - start).days * 24\n",
    "collection = [None] * collection_size\n",
    "#Select the data from March to June\n",
    "def get_Data(beijing_weather_data_value,collection,collection_size):\n",
    "    print(collection_size)\n",
    "    cursor = 0\n",
    "    for index in tqdm(range(beijing_weather_data_value.shape[0])):\n",
    "        for lat in lat_step:\n",
    "            for lon in lon_step:\n",
    "                if (beijing_weather_data_value[index][1] == lon and \n",
    "                    beijing_weather_data_value[index][2] == lat and\n",
    "                    beijing_weather_data_value[index][3].month >= 3 and\n",
    "                    beijing_weather_data_value[index][3].month <= 6):\n",
    "                    collection[cursor] = beijing_weather_data_value[index]\n",
    "                    cursor = cursor + 1\n",
    "                    if cursor == collection_size:\n",
    "                        print(cursor)\n",
    "                        return collection;\n",
    "#Extract all the data from CSV\n",
    "collection = get_Data(beijing_weather_data_value,collection,collection_size)\n",
    "with open(\"1hour.pkl\",\"wb\") as file:\n",
    "    pickle.dump(collection, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('ok!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beijing_grid_048' 115.2 39.6 datetime.datetime(2017, 3, 1, 0, 0) -4.4\n",
      " 916.13 35.47 333.37 22.56]\n",
      "{'_id': ObjectId('5ae019a950fc202dcd53e02f'), 'longitude': 115.29411764705883, 'latitude': 39.986486486486484, 'elevation': 1600.509399414062}\n"
     ]
    }
   ],
   "source": [
    "#Read the data\n",
    "import pickle\n",
    "collection = list()\n",
    "with open(\"1hour.pkl\",\"rb\") as file:\n",
    "    collection = pickle.load(file)\n",
    "print(collection[0])\n",
    "#Read height data\n",
    "with open(\"All_height_data_1hr.pk\",\"rb\") as file:\n",
    "    height_data = pickle.load(file)\n",
    "#print(round(height_data[-1]['latitude'],1))\n",
    "print(height_data[-1])\n",
    "with open(\"PM25_Realdata.pkl\",\"rb\") as file:\n",
    "    PM25_real_data = pk.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "22\n",
      "{39.6: 0, 39.7: 1, 39.8: 2, 39.9: 3, 40.0: 4, 40.1: 5, 40.2: 6, 40.3: 7}\n"
     ]
    }
   ],
   "source": [
    "lat_step_dict = {lat_step[i]:i for i in range(len(lat_step))}\n",
    "print(len(lat_step_dict))\n",
    "lon_step_dict = {lon_step[i]:i for i in range(len(lon_step))}\n",
    "print(len(lon_step_dict))\n",
    "print(lat_step_dict)\n",
    "Height_map = np.zeros(shape = (len(lon_step),len(lat_step)))\n",
    "for data in height_data:\n",
    "    lat_index = lat_step_dict[round(data['latitude'],1)]\n",
    "    lon_index = lon_step_dict[round(data['longitude'],1)]\n",
    "    if not Height_map[lon_index][lat_index] == 0:\n",
    "        Height_map[lon_index][lat_index] = (\n",
    "            Height_map[lon_index][lat_index] + data['elevation'])\n",
    "    else:\n",
    "        Height_map[lon_index][lat_index] = data['elevation']\n",
    "#print(Height_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-91bc2963c187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#collection[:,3] = datetime.strptime('%y-%m-%d %H:%M:%S',collection[:,3])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collection' is not defined"
     ]
    }
   ],
   "source": [
    "#Prepare datas for SVR\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "print(collection[0][3])\n",
    "#collection[:,3] = datetime.strptime('%y-%m-%d %H:%M:%S',collection[:,3])\n",
    "lon = np.zeros(shape = (len(collection)))\n",
    "lat = np.zeros(shape = (len(collection)))\n",
    "month = np.zeros(shape = (len(collection)))\n",
    "date = np.zeros(shape = (len(collection)))\n",
    "hour = np.zeros(shape = (len(collection)))\n",
    "temp = np.zeros(shape = (len(collection)))\n",
    "humid = np.zeros(shape = (len(collection)))\n",
    "press = np.zeros(shape = (len(collection)))\n",
    "direct = np.zeros(shape = (len(collection)))\n",
    "speed = np.zeros(shape = (len(collection)))\n",
    "for index in tqdm(range(len(collection))):\n",
    "    lon[index] = collection[index][1]\n",
    "    lat[index] = collection[index][2]\n",
    "    month[index] = collection[index][3].month\n",
    "    date[index] = collection[index][3].day\n",
    "    hour[index] = collection[index][3].hour\n",
    "    temp[index] = collection[index][4]\n",
    "    humid[index] = collection[index][6]\n",
    "    press[index] = collection[index][5]\n",
    "    direct[index] = collection[index][7]\n",
    "    speed[index] = collection[index][8]\n",
    "lon = np.expand_dims(lon,axis = 1)\n",
    "lat = np.expand_dims(lat,axis = 1)\n",
    "month = np.expand_dims(month,axis = 1)\n",
    "date = np.expand_dims(date,axis = 1)\n",
    "hour = np.expand_dims(hour,axis = 1)\n",
    "temp = np.expand_dims(temp,axis = 1)\n",
    "humid = np.expand_dims(humid,axis = 1)\n",
    "press = np.expand_dims(press,axis = 1)\n",
    "direct = np.expand_dims(direct,axis = 1)\n",
    "speed = np.expand_dims(speed,axis = 1)\n",
    "X = np.concatenate((lon,lat,month,date,hour,temp,humid,press,direct,speed),axis = 1)\n",
    "with open(\"1hour_prepared.pkl\",\"wb\") as file:\n",
    "    pickle.dump(X, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[115.2   39.6    3.     1.     0.    -4.4   35.47 916.13 333.37  22.56]\n",
      " [115.2   39.7    3.     1.     0.    -5.79  39.06 905.54 332.07  25.16]\n",
      " [115.2   39.8    3.     1.     0.    -7.14  42.96 892.6  330.67  26.78]\n",
      " [115.2   39.9    3.     1.     0.    -8.44  47.44 874.98 328.6   26.46]\n",
      " [115.2   40.     3.     1.     0.    -9.73  51.93 857.35 326.49  26.17]\n",
      " [115.2   40.1    3.     1.     0.    -8.61  48.58 880.25 324.21  27.03]\n",
      " [115.2   40.2    3.     1.     0.    -7.48  45.24 903.14 322.07  27.92]\n",
      " [115.2   40.3    3.     1.     0.    -6.74  42.42 917.75 322.45  28.27]\n",
      " [115.3   39.6    3.     1.     0.    -2.54  32.41 939.33 337.13  17.22]\n",
      " [115.3   39.7    3.     1.     0.    -3.58  33.74 938.19 334.49  21.71]]\n",
      "(515328, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.經度(longt)\\n1.緯度(lat)\\n2.月份(month)\\n3.小時(hour)\\n4.溫度(temp)\\n5.濕度(humidity)\\n6.氣壓(pressure)\\n7.風向(wind_direction)\\n8.風速(wind_speed)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deserialize all the raw grid data\n",
    "import pickle\n",
    "with open(\"1hour_prepared.pkl\",\"rb\") as file:\n",
    "    X = pickle.load(file)\n",
    "print(X[:10])\n",
    "print(X.shape)\n",
    "'''0.經度(longt)\n",
    "1.緯度(lat)\n",
    "2.月份(month)\n",
    "3.小時(hour)\n",
    "4.溫度(temp)\n",
    "5.濕度(humidity)\n",
    "6.氣壓(pressure)\n",
    "7.風向(wind_direction)\n",
    "8.風速(wind_speed)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#SVR analysis\n",
    "'''X => SVR輸入向量的numpy array，資料必須照著下面的順序:\n",
    "0.經度(longt)\n",
    "1.緯度(lat)\n",
    "2.月份(month)\n",
    "3.天數(day)\n",
    "4.小時(hour)\n",
    "5.溫度(temp)\n",
    "6.濕度(humidity)\n",
    "7.氣壓(pressure)\n",
    "8.風向(wind_direction)\n",
    "9.風速(wind_speed)\n",
    "Y => PM2.5的預測值'''\n",
    "import pickle\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "\n",
    "svr = joblib.load('SVR_beijing.pkl')\n",
    "X = np.array(X, dtype=np.float64)\n",
    "X_mu = np.array([116.40616279,39.98908696,4.45972151,14.88423765,11.40249715,17.0021192,33.65642229,996.1066705,208.34196307,10.85087688])\n",
    "X_sigma = np.array([0.28161015,0.22815651,1.14854307,8.88446275,6.90495716,8.44671392,19.41044621,19.83802731,96.65489494,6.85870324])\n",
    "Y_mu = 59.541867360598516\n",
    "Y_sigma = 61.21714698899477\n",
    "\n",
    "Xs = (X - X_mu) / X_sigma #對要輸入的X向量標準化\n",
    "Y_pdt = svr.predict(Xs) #將標準化後的X向量放入SVR預測\n",
    "Y = Y_pdt * Y_sigma + Y_mu #對SVR得到的預測值反標準化\n",
    "print(Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "with open(\"1_hr_pm25.pkl\",\"wb\") as file:\n",
    "    pk.dump(Y, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize input data\n",
    "def normalize(x):\n",
    "    norm = (x - x.min())/(x.max() - x.min())\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2928\n"
     ]
    }
   ],
   "source": [
    "print(len(PM25_real_data[116.0][39.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ( 115.2 , 39.6 ) exists\n",
      "No ( 115.2 , 39.7 ) exists\n",
      "No ( 115.2 , 39.8 ) exists\n",
      "No ( 115.2 , 39.9 ) exists\n",
      "No ( 115.2 , 40.0 ) exists\n",
      "No ( 115.2 , 40.1 ) exists\n",
      "No ( 115.2 , 40.2 ) exists\n",
      "No ( 115.2 , 40.3 ) exists\n",
      "No ( 115.3 , 39.6 ) exists\n",
      "No ( 115.3 , 39.7 ) exists\n",
      "No ( 115.3 , 39.8 ) exists\n",
      "No ( 115.3 , 39.9 ) exists\n",
      "No ( 115.3 , 40.0 ) exists\n",
      "No ( 115.3 , 40.1 ) exists\n",
      "No ( 115.3 , 40.2 ) exists\n",
      "No ( 115.3 , 40.3 ) exists\n",
      "No ( 115.4 , 39.6 ) exists\n",
      "No ( 115.4 , 39.7 ) exists\n",
      "No ( 115.4 , 39.8 ) exists\n",
      "No ( 115.4 , 39.9 ) exists\n",
      "No ( 115.4 , 40.0 ) exists\n",
      "No ( 115.4 , 40.1 ) exists\n",
      "No ( 115.4 , 40.2 ) exists\n",
      "No ( 115.4 , 40.3 ) exists\n",
      "No ( 115.5 , 39.6 ) exists\n",
      "No ( 115.5 , 39.7 ) exists\n",
      "No ( 115.5 , 39.8 ) exists\n",
      "No ( 115.5 , 39.9 ) exists\n",
      "No ( 115.5 , 40.0 ) exists\n",
      "No ( 115.5 , 40.1 ) exists\n",
      "No ( 115.5 , 40.2 ) exists\n",
      "No ( 115.5 , 40.3 ) exists\n",
      "No ( 115.6 , 39.6 ) exists\n",
      "No ( 115.6 , 39.7 ) exists\n",
      "No ( 115.6 , 39.8 ) exists\n",
      "No ( 115.6 , 39.9 ) exists\n",
      "No ( 115.6 , 40.0 ) exists\n",
      "No ( 115.6 , 40.1 ) exists\n",
      "No ( 115.6 , 40.2 ) exists\n",
      "No ( 115.6 , 40.3 ) exists\n",
      "No ( 115.7 , 39.6 ) exists\n",
      "No ( 115.7 , 39.7 ) exists\n",
      "No ( 115.7 , 39.8 ) exists\n",
      "No ( 115.7 , 39.9 ) exists\n",
      "No ( 115.7 , 40.0 ) exists\n",
      "No ( 115.7 , 40.1 ) exists\n",
      "No ( 115.7 , 40.2 ) exists\n",
      "No ( 115.7 , 40.3 ) exists\n",
      "No ( 115.8 , 39.6 ) exists\n",
      "No ( 115.8 , 39.7 ) exists\n",
      "No ( 115.8 , 39.8 ) exists\n",
      "No ( 115.8 , 39.9 ) exists\n",
      "No ( 115.8 , 40.0 ) exists\n",
      "No ( 115.8 , 40.1 ) exists\n",
      "No ( 115.8 , 40.2 ) exists\n",
      "No ( 115.8 , 40.3 ) exists\n",
      "No ( 115.9 , 39.6 ) exists\n",
      "No ( 115.9 , 39.7 ) exists\n",
      "No ( 115.9 , 39.8 ) exists\n",
      "No ( 115.9 , 39.9 ) exists\n",
      "No ( 115.9 , 40.0 ) exists\n",
      "No ( 115.9 , 40.1 ) exists\n",
      "No ( 115.9 , 40.2 ) exists\n",
      "No ( 115.9 , 40.3 ) exists\n",
      "No ( 117.0 , 39.6 ) exists\n",
      "No ( 117.0 , 39.7 ) exists\n",
      "No ( 117.0 , 39.8 ) exists\n",
      "No ( 117.0 , 39.9 ) exists\n",
      "No ( 117.0 , 40.0 ) exists\n",
      "No ( 117.0 , 40.1 ) exists\n",
      "No ( 117.0 , 40.2 ) exists\n",
      "No ( 117.0 , 40.3 ) exists\n",
      "No ( 117.2 , 39.6 ) exists\n",
      "No ( 117.2 , 39.7 ) exists\n",
      "No ( 117.2 , 39.8 ) exists\n",
      "No ( 117.2 , 39.9 ) exists\n",
      "No ( 117.2 , 40.0 ) exists\n",
      "No ( 117.2 , 40.1 ) exists\n",
      "No ( 117.2 , 40.2 ) exists\n",
      "No ( 117.2 , 40.3 ) exists\n",
      "No ( 117.3 , 39.6 ) exists\n",
      "No ( 117.3 , 39.7 ) exists\n",
      "No ( 117.3 , 39.8 ) exists\n",
      "No ( 117.3 , 39.9 ) exists\n",
      "No ( 117.3 , 40.0 ) exists\n",
      "No ( 117.3 , 40.1 ) exists\n",
      "No ( 117.3 , 40.2 ) exists\n",
      "No ( 117.3 , 40.3 ) exists\n",
      "(2928, 22, 8, 1)\n",
      "(2928, 22, 8, 1)\n",
      "(2928, 22, 8, 6)\n",
      "ok!\n"
     ]
    }
   ],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "#load all the prepared data for SVR(to add feature to )\n",
    "with open(\"1hour_prepared.pkl\",\"rb\") as file:\n",
    "    X = pk.load(file)\n",
    "#load all the SVR data\n",
    "with open(\"1_hr_pm25.pkl\",\"rb\") as file:\n",
    "    Y = pk.load(file)\n",
    "#load the ercise point of latitude and longitude\n",
    "with open(\"lat_lon_range.pkl\",\"rb\") as file:\n",
    "    lat_step, lon_step = pk.load(file)\n",
    "#Y transform to [date][lon][lat]\n",
    "date_count = (date(2017,7,1) - date(2017,3,1)).days\n",
    "Y = np.reshape(Y, (24 * date_count,len(lon_step),len(lat_step),1))\n",
    "#Use the lon_step and lat_step to determin whether there is a real data\n",
    "for longitude in lon_step:#Go through all the longitude in the lon_step\n",
    "    for latitude in lat_step:#Go throungh all the latitude in the lat_step\n",
    "        try: #Try the first element\n",
    "            PM25_real_data[longitude][latitude][0]\n",
    "            for i in range(24 * date_count):\n",
    "                if not PM25_real_data[longitude][latitude][i] == 0:\n",
    "                    Y[i][lon_step_dict[longitude]][lat_step_dict[latitude]] = PM25_real_data[longitude][latitude][i]\n",
    "        except:\n",
    "            print(\"No (\",longitude,\",\",latitude,\") exists\")\n",
    "            \n",
    "Y[:] = normalize(Y[:])\n",
    "Height_map = np.expand_dims(Height_map,axis = 0)\n",
    "Height_map = np.repeat(Height_map,24*date_count,axis = 0)\n",
    "Height_map = np.expand_dims(Height_map,axis = 3)\n",
    "print(Height_map.shape)\n",
    "#Get all the other feature\n",
    "temp = np.reshape(X[:,5],Y.shape)\n",
    "print(temp.shape)\n",
    "humidity = np.reshape(X[:,6],Y.shape)\n",
    "pressure = np.reshape(X[:,7],Y.shape)\n",
    "wind_speed = np.reshape(X[:,9],Y.shape)\n",
    "temp[:] = normalize(temp[:])\n",
    "humidity[:] = normalize(humidity[:])\n",
    "pressure[:] = normalize(pressure[:])\n",
    "wind_speed[:] = normalize(wind_speed[:])\n",
    "Height_map[:] = normalize(Height_map[:])\n",
    "Y = np.concatenate((Y,temp,humidity,pressure,wind_speed,Height_map),axis = 3)\n",
    "print(Y.shape)\n",
    "#Get the longitude of the target site\n",
    "target_lon = np.argwhere(lon_step == 116.3)\n",
    "#Get the latitude of the target site\n",
    "target_lat = np.argwhere(lat_step == 39.9)\n",
    "#There  are 256 objects in pickle file and there are 10 pickle files\n",
    "validation_size = len(Y) - 2560 \n",
    "#Dump validation data\n",
    "with open(\"1hour_pm25_validation_array.pkl\",\"wb\") as file:\n",
    "    pk.dump((Y[-validation_size - 1:-1],Y[-validation_size:,target_lon,target_lat,0]),file)\n",
    "#Dump testing data\n",
    "#Assume that batch size is 256\n",
    "for index in range(1, int(len(Y) / 256)):\n",
    "    with open(\"batch_number_\" + str(index) + \".pkl\",\"wb\") as file:\n",
    "        if 256 * index >= len(Y):\n",
    "            end_batch = len(Y) - 1\n",
    "        else:\n",
    "            end_batch = 256 * index\n",
    "        pk.dump((Y[256 * (index - 1):end_batch],#Testing batch\n",
    "                 Y[256 * (index - 1) + 1:end_batch + 1 ,target_lon,target_lat,0]),file)#Testing label\n",
    "print(\"ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start building network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Validation Shape\n",
      "(368, 22, 8, 6)\n",
      "(368, 1, 1)\n",
      "================\n",
      "Testing Shape\n",
      "(10, 256, 22, 8, 6)\n",
      "(10, 256, 1, 1)\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "#print(validation_size)\n",
    "batch = list()\n",
    "batch_label = list()\n",
    "#Serialize all the batch\n",
    "for index in range(1,11):\n",
    "    with open(\"batch_number_\" + str(index) + \".pkl\",\"rb\") as file:\n",
    "        batch_read, batch_label_read = pk.load(file)\n",
    "        batch.append(batch_read)\n",
    "        batch_label.append(batch_label_read)\n",
    "#Serialize the area \n",
    "with open(\"lat_lon_range.pkl\",\"rb\") as file:\n",
    "    lat_step, lon_step = pk.load(file)\n",
    "\n",
    "with open(\"1hour_pm25_validation_array.pkl\",\"rb\") as file:\n",
    "    X_validation, Y_validation = pk.load(file)\n",
    "print(\"================\")\n",
    "print(\"Validation Shape\")\n",
    "print(X_validation.shape)\n",
    "print(Y_validation.shape)\n",
    "print(\"================\")\n",
    "#print(X_validation[-1])\n",
    "#print(Y_validation[-2])\n",
    "batch = np.array(batch)\n",
    "batch_label = np.array(batch_label)\n",
    "print(\"Testing Shape\")\n",
    "print(batch.shape)\n",
    "print(batch_label.shape)\n",
    "print(\"================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_layer(x_tensor, conv_num_outputs, conv_ksize, conv_strides):\n",
    "    x_shape = x_tensor.get_shape().as_list()\n",
    "    weights = tf.Variable(tf.truncated_normal(\n",
    "        [*conv_ksize[:],x_shape[-1],conv_num_outputs],\n",
    "        mean = 0, stddev = 0.1))\n",
    "    strides = [1,*conv_strides[:],1]\n",
    "    padding = \"SAME\"\n",
    "    convolution = tf.nn.conv2d(x_tensor,weights,strides,padding)\n",
    "    #No batch norm\n",
    "    #convolution = tf.layers.batch_normalization(convolution, training=training)\n",
    "    convolution = tf.nn.relu(convolution)\n",
    "    print(\"--------------\")\n",
    "    print(\"conv_2d\")\n",
    "    print(convolution.get_shape().as_list())\n",
    "    tf.summary.histogram(\"weights\",weights)\n",
    "    return convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Build_CNN(x):\n",
    "    with tf.name_scope(\"CNN_1\"):\n",
    "        CNN = tf.layers.conv2d(inputs = x,filters = 64,kernel_size = 3,padding = 'SAME',activation = tf.nn.relu)\n",
    "        CNN = tf.nn.relu(CNN)\n",
    "        #CNN = CNN_layer(x,64,(3,3),(1,1))\n",
    "    with tf.name_scope(\"CNN_2\"):\n",
    "        CNN = tf.layers.conv2d(inputs = x,filters = 64,kernel_size = 3,padding = 'SAME',activation = tf.nn.relu)\n",
    "        CNN = tf.nn.relu(CNN)\n",
    "        #CNN = CNN_layer(x,64,(3,3),(1,1))\n",
    "    with tf.name_scope(\"flatten\"):\n",
    "        CNN = tf.contrib.layers.flatten(CNN)\n",
    "    with tf.name_scope(\"FC1\"):\n",
    "        CNN = tf.contrib.layers.fully_connected(CNN,128)\n",
    "        CNN = tf.nn.relu(CNN)\n",
    "    with tf.name_scope(\"FC2\"):\n",
    "        CNN = tf.contrib.layers.fully_connected(CNN,128)\n",
    "        CNN = tf.nn.relu(CNN)\n",
    "    with tf.name_scope(\"output\"):\n",
    "        CNN = tf.contrib.layers.fully_connected(CNN,48)\n",
    "    return CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_loss(session,validation_batch,validation_label,loss):\n",
    "    loss = session.run(loss,feed_dict = {x:validation_batch,y:validation_label})\n",
    "    print(\"|loss:{:.7f}\".format(loss),\"|\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Reset the graph\n",
    "tf.reset_default_graph()\n",
    "#Input of the network\n",
    "x = tf.placeholder(tf.float32,shape = (None,len(lon_step),len(lat_step),6), name = 'x')\n",
    "y = tf.placeholder(tf.float32,shape = (None),name = 'y')\n",
    "#Build the network\n",
    "logits = Build_CNN(x)\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "#Define Loss and optimizer\n",
    "with tf.name_scope(\"loss\"):\n",
    "    a = tf.abs(tf.subtract(y,logits))\n",
    "    b = tf.add(y,logits)\n",
    "    loss = tf.scalar_mul(2, tf.reduce_mean(tf.divide(a, b)))\n",
    "    #loss = tf.losses.mean_squared_error(y,logits)\n",
    "tf.summary.scalar(\"loss\",loss)\n",
    "#MSE version \n",
    "#loss = tf.sqrt(tf.reduce_mean(tf.losses.mean_squared_error(labels = y,predictions = logits)))\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "merged = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch = 10000\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"log/pm25+temp\", graph = sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epoch):\n",
    "        for batch_num in range(0,10):\n",
    "            sess.run(optimizer,feed_dict = {x:batch[batch_num],y:batch_label[batch_num]})\n",
    "            #print(\"logis:\")\n",
    "            #print(logits.eval({x:X_validation}))\n",
    "            #print(\"Y:\")\n",
    "            #print(Y_validation)\n",
    "            loss_value = sess.run(loss,feed_dict = {x:X_validation,y:Y_validation})\n",
    "        if e % 30 == 0:\n",
    "            result = sess.run(merged,feed_dict = {x:X_validation,y:Y_validation})\n",
    "            writer.add_summary(result,e)\n",
    "            print(\"|epoch:\",e,\"|loss:\",loss_value,\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
