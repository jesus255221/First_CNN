{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/CN_AQ/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.461 116.146 116.30417\n",
      "40.09 39.824 39.935085\n"
     ]
    }
   ],
   "source": [
    "Urban_station_lonlats = np.array([\n",
    "['dongsi_aq',116.417,39.929],\n",
    "['tiantan_aq',116.407,39.886],\n",
    "['guanyuan_aq',116.339,39.929],\n",
    "['wanshouxigong_aq',116.352,39.878],\n",
    "['aotizhongxin_aq',116.397,39.982],\n",
    "['nongzhanguan_aq',116.461,39.937],\n",
    "['wanliu_aq',116.287,39.987],\n",
    "['beibuxinqu_aq',116.174,40.09],\n",
    "['zhiwuyuan_aq',116.207,40.002],\n",
    "['fengtaihuayuan_aq',116.279,39.863],\n",
    "['yungang_aq',116.146,39.824],\n",
    "['gucheng_aq',116.184,39.914]\n",
    "])\n",
    "lons = np.array(Urban_station_lonlats[:,1],dtype = np.float32)\n",
    "lats = np.array(Urban_station_lonlats[:,2],dtype = np.float32)\n",
    "print(lons.max(),lons.min(),lons.mean())\n",
    "print(lats.max(),lats.min(),lats.mean())\n",
    "X_length = 93.6\n",
    "Y_length = 41.2\n",
    "#Change X Y to degree\n",
    "Y_degree = Y_length / 111\n",
    "#Consider the length of the latitude line\n",
    "X_degree = X_length / (40000 * np.cos(lats.mean() / 180 * 3.1415926) / 360)\n",
    "#X_degree = X_length / 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39.6 39.7 39.8 39.9 40.  40.1 40.2 40.3]\n",
      "[115.2 115.3 115.4 115.5 115.6 115.7 115.8 115.9 116.  116.1 116.2 116.3\n",
      " 116.4 116.5 116.6 116.7 116.8 116.9 117.  117.1 117.2 117.3]\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "#Initialize the step fo building \n",
    "lat_step = np.arange(lats.mean() - Y_degree,lats.mean() + Y_degree,.1)\n",
    "lon_step = np.arange(lons.mean() - X_degree,lons.mean() + X_degree,.1)\n",
    "#Build the step map\n",
    "for index in range(len(lat_step)):\n",
    "    lat_step[index] = round(lat_step[index],1)\n",
    "for index in range(len(lon_step)):\n",
    "    lon_step[index] = round(lon_step[index],1)\n",
    "print(lat_step)\n",
    "print(lon_step)\n",
    "#Get the size of point waiting to be analyzed\n",
    "point_size = len(lat_step) * len(lon_step)\n",
    "with open(\"lat_lon_range.pkl\",\"wb\") as file:\n",
    "    pickle.dump((lat_step,lon_step),file)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        stationName  longitude  latitude             utc_time  temperature  \\\n",
      "0  beijing_grid_000      115.0      39.0  2017-01-01 00:00:00        -5.47   \n",
      "1  beijing_grid_001      115.0      39.1  2017-01-01 00:00:00        -5.53   \n",
      "2  beijing_grid_002      115.0      39.2  2017-01-01 00:00:00        -5.70   \n",
      "3  beijing_grid_003      115.0      39.3  2017-01-01 00:00:00        -5.88   \n",
      "4  beijing_grid_004      115.0      39.4  2017-01-01 00:00:00        -5.34   \n",
      "\n",
      "   pressure  humidity  wind_direction  wind_speed/kph  \n",
      "0    984.73     76.60           53.71            3.53  \n",
      "1    979.33     75.40           43.59            3.11  \n",
      "2    963.14     71.80            0.97            2.75  \n",
      "3    946.94     68.20          327.65            3.84  \n",
      "4    928.80     58.81          317.85            6.14  \n"
     ]
    }
   ],
   "source": [
    "#Use panda to read csv\n",
    "beijing_weather_data = pd.read_csv(\"Beijing_historical_meo_grid.csv\")\n",
    "print(beijing_weather_data.head())\n",
    "#Get the value of beijing data\n",
    "beijing_weather_data_value = beijing_weather_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from datetime import datetime\n",
    "#Build the datetime object for date delta calculation\n",
    "for index in tqdm(range(len(beijing_weather_data_value))):\n",
    "    beijing_weather_data_value[index][3] = datetime.strptime(\n",
    "        beijing_weather_data_value[index][3],'%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Caculate the days between June 31 to March 1\n",
    "start = date(2017,3,1)\n",
    "end = date(2017,7,1)\n",
    "#Set the correct array size\n",
    "collection_size = point_size * (end - start).days * 24\n",
    "collection = [None] * collection_size\n",
    "#Select the data from March to June\n",
    "def get_Data(beijing_weather_data_value,collection,collection_size):\n",
    "    print(collection_size)\n",
    "    cursor = 0\n",
    "    for index in tqdm(range(beijing_weather_data_value.shape[0])):\n",
    "        for lat in lat_step:\n",
    "            for lon in lon_step:\n",
    "                if (beijing_weather_data_value[index][1] == lon and \n",
    "                    beijing_weather_data_value[index][2] == lat and\n",
    "                    beijing_weather_data_value[index][3].month >= 3 and\n",
    "                    beijing_weather_data_value[index][3].month <= 6):\n",
    "                    collection[cursor] = beijing_weather_data_value[index]\n",
    "                    cursor = cursor + 1\n",
    "                    if cursor == collection_size:\n",
    "                        print(cursor)\n",
    "                        return collection;\n",
    "#Extract all the data from CSV\n",
    "collection = get_Data(beijing_weather_data_value,collection,collection_size)\n",
    "with open(\"1hour.pkl\",\"wb\") as file:\n",
    "    pickle.dump(collection, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('ok!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beijing_grid_048' 115.2 39.6 datetime.datetime(2017, 3, 1, 0, 0) -4.4\n",
      " 916.13 35.47 333.37 22.56]\n",
      "{'_id': ObjectId('5ae019a950fc202dcd53e02f'), 'longitude': 115.29411764705883, 'latitude': 39.986486486486484, 'elevation': 1600.509399414062}\n"
     ]
    }
   ],
   "source": [
    "#Read the data\n",
    "import pickle\n",
    "collection = list()\n",
    "with open(\"1hour.pkl\",\"rb\") as file:\n",
    "    collection = pickle.load(file)\n",
    "print(collection[0])\n",
    "#Read height data\n",
    "with open(\"All_height_data_1hr.pk\",\"rb\") as file:\n",
    "    height_data = pickle.load(file)\n",
    "#print(round(height_data[-1]['latitude'],1))\n",
    "print(height_data[-1])\n",
    "with open(\"PM25_Realdata.pkl\",\"rb\") as file:\n",
    "    PM25_real_data = pk.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "22\n",
      "{39.6: 0, 39.7: 1, 39.8: 2, 39.9: 3, 40.0: 4, 40.1: 5, 40.2: 6, 40.3: 7}\n"
     ]
    }
   ],
   "source": [
    "lat_step_dict = {lat_step[i]:i for i in range(len(lat_step))}\n",
    "print(len(lat_step_dict))\n",
    "lon_step_dict = {lon_step[i]:i for i in range(len(lon_step))}\n",
    "print(len(lon_step_dict))\n",
    "print(lat_step_dict)\n",
    "Height_map = np.zeros(shape = (len(lon_step),len(lat_step)))\n",
    "for data in height_data:\n",
    "    lat_index = lat_step_dict[round(data['latitude'],1)]\n",
    "    lon_index = lon_step_dict[round(data['longitude'],1)]\n",
    "    if not Height_map[lon_index][lat_index] == 0:\n",
    "        Height_map[lon_index][lat_index] = (\n",
    "            Height_map[lon_index][lat_index] + data['elevation'])\n",
    "    else:\n",
    "        Height_map[lon_index][lat_index] = data['elevation']\n",
    "#print(Height_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-91bc2963c187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#collection[:,3] = datetime.strptime('%y-%m-%d %H:%M:%S',collection[:,3])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collection' is not defined"
     ]
    }
   ],
   "source": [
    "#Prepare datas for SVR\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "print(collection[0][3])\n",
    "#collection[:,3] = datetime.strptime('%y-%m-%d %H:%M:%S',collection[:,3])\n",
    "lon = np.zeros(shape = (len(collection)))\n",
    "lat = np.zeros(shape = (len(collection)))\n",
    "month = np.zeros(shape = (len(collection)))\n",
    "date = np.zeros(shape = (len(collection)))\n",
    "hour = np.zeros(shape = (len(collection)))\n",
    "temp = np.zeros(shape = (len(collection)))\n",
    "humid = np.zeros(shape = (len(collection)))\n",
    "press = np.zeros(shape = (len(collection)))\n",
    "direct = np.zeros(shape = (len(collection)))\n",
    "speed = np.zeros(shape = (len(collection)))\n",
    "for index in tqdm(range(len(collection))):\n",
    "    lon[index] = collection[index][1]\n",
    "    lat[index] = collection[index][2]\n",
    "    month[index] = collection[index][3].month\n",
    "    date[index] = collection[index][3].day\n",
    "    hour[index] = collection[index][3].hour\n",
    "    temp[index] = collection[index][4]\n",
    "    humid[index] = collection[index][6]\n",
    "    press[index] = collection[index][5]\n",
    "    direct[index] = collection[index][7]\n",
    "    speed[index] = collection[index][8]\n",
    "lon = np.expand_dims(lon,axis = 1)\n",
    "lat = np.expand_dims(lat,axis = 1)\n",
    "month = np.expand_dims(month,axis = 1)\n",
    "date = np.expand_dims(date,axis = 1)\n",
    "hour = np.expand_dims(hour,axis = 1)\n",
    "temp = np.expand_dims(temp,axis = 1)\n",
    "humid = np.expand_dims(humid,axis = 1)\n",
    "press = np.expand_dims(press,axis = 1)\n",
    "direct = np.expand_dims(direct,axis = 1)\n",
    "speed = np.expand_dims(speed,axis = 1)\n",
    "X = np.concatenate((lon,lat,month,date,hour,temp,humid,press,direct,speed),axis = 1)\n",
    "with open(\"1hour_prepared.pkl\",\"wb\") as file:\n",
    "    pickle.dump(X, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[115.2   39.6    3.     1.     0.    -4.4   35.47 916.13 333.37  22.56]\n",
      " [115.2   39.7    3.     1.     0.    -5.79  39.06 905.54 332.07  25.16]\n",
      " [115.2   39.8    3.     1.     0.    -7.14  42.96 892.6  330.67  26.78]\n",
      " [115.2   39.9    3.     1.     0.    -8.44  47.44 874.98 328.6   26.46]\n",
      " [115.2   40.     3.     1.     0.    -9.73  51.93 857.35 326.49  26.17]\n",
      " [115.2   40.1    3.     1.     0.    -8.61  48.58 880.25 324.21  27.03]\n",
      " [115.2   40.2    3.     1.     0.    -7.48  45.24 903.14 322.07  27.92]\n",
      " [115.2   40.3    3.     1.     0.    -6.74  42.42 917.75 322.45  28.27]\n",
      " [115.3   39.6    3.     1.     0.    -2.54  32.41 939.33 337.13  17.22]\n",
      " [115.3   39.7    3.     1.     0.    -3.58  33.74 938.19 334.49  21.71]]\n",
      "(515328, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.經度(longt)\\n1.緯度(lat)\\n2.月份(month)\\n3.小時(hour)\\n4.溫度(temp)\\n5.濕度(humidity)\\n6.氣壓(pressure)\\n7.風向(wind_direction)\\n8.風速(wind_speed)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deserialize all the raw grid data\n",
    "import pickle\n",
    "with open(\"1hour_prepared.pkl\",\"rb\") as file:\n",
    "    X = pickle.load(file)\n",
    "print(X[:10])\n",
    "print(X.shape)\n",
    "'''0.經度(longt)\n",
    "1.緯度(lat)\n",
    "2.月份(month)\n",
    "3.小時(hour)\n",
    "4.溫度(temp)\n",
    "5.濕度(humidity)\n",
    "6.氣壓(pressure)\n",
    "7.風向(wind_direction)\n",
    "8.風速(wind_speed)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#SVR analysis\n",
    "'''X => SVR輸入向量的numpy array，資料必須照著下面的順序:\n",
    "0.經度(longt)\n",
    "1.緯度(lat)\n",
    "2.月份(month)\n",
    "3.天數(day)\n",
    "4.小時(hour)\n",
    "5.溫度(temp)\n",
    "6.濕度(humidity)\n",
    "7.氣壓(pressure)\n",
    "8.風向(wind_direction)\n",
    "9.風速(wind_speed)\n",
    "Y => PM2.5的預測值'''\n",
    "import pickle\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "\n",
    "svr = joblib.load('SVR_beijing.pkl')\n",
    "X = np.array(X, dtype=np.float64)\n",
    "X_mu = np.array([116.40616279,39.98908696,4.45972151,14.88423765,11.40249715,17.0021192,33.65642229,996.1066705,208.34196307,10.85087688])\n",
    "X_sigma = np.array([0.28161015,0.22815651,1.14854307,8.88446275,6.90495716,8.44671392,19.41044621,19.83802731,96.65489494,6.85870324])\n",
    "Y_mu = 59.541867360598516\n",
    "Y_sigma = 61.21714698899477\n",
    "\n",
    "Xs = (X - X_mu) / X_sigma #對要輸入的X向量標準化\n",
    "Y_pdt = svr.predict(Xs) #將標準化後的X向量放入SVR預測\n",
    "Y = Y_pdt * Y_sigma + Y_mu #對SVR得到的預測值反標準化\n",
    "print(Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "with open(\"1_hr_pm25.pkl\",\"wb\") as file:\n",
    "    pk.dump(Y, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize input data\n",
    "def normalize(x):\n",
    "    norm = (x - x.min())/(x.max() - x.min())\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2928\n"
     ]
    }
   ],
   "source": [
    "print(len(PM25_real_data[116.0][39.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "22\n",
      "{39.6: 0, 39.7: 1, 39.8: 2, 39.9: 3, 40.0: 4, 40.1: 5, 40.2: 6, 40.3: 7}\n",
      "(2928, 22, 8, 6)\n",
      "(320, 48)\n",
      "(256, 48)\n",
      "(256, 48)\n",
      "(256, 48)\n",
      "(256, 48)\n",
      "(256, 48)\n",
      "(256, 48)\n",
      "(256, 48)\n",
      "(256, 48)\n",
      "(256, 48)\n",
      "(256, 48)\n",
      "ok!\n"
     ]
    }
   ],
   "source": [
    "#Normalize input data\n",
    "def normalize(x):\n",
    "    norm = (x - x.min())/(x.max() - x.min())\n",
    "    return norm\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "#load all the prepared data for SVR(to add feature to )\n",
    "with open(\"1hour_prepared.pkl\",\"rb\") as file:\n",
    "    X = pk.load(file)\n",
    "#load all the SVR data\n",
    "with open(\"1_hr_pm25.pkl\",\"rb\") as file:\n",
    "    Y = pk.load(file)\n",
    "#load the ercise point of latitude and longitude\n",
    "with open(\"lat_lon_range.pkl\",\"rb\") as file:\n",
    "    lat_step, lon_step = pk.load(file)\n",
    "#Read height data\n",
    "with open(\"All_height_data_1hr.pk\",\"rb\") as file:\n",
    "    height_data = pk.load(file)\n",
    "#Process Height data\n",
    "lat_step_dict = {lat_step[i]:i for i in range(len(lat_step))}\n",
    "print(len(lat_step_dict))\n",
    "lon_step_dict = {lon_step[i]:i for i in range(len(lon_step))}\n",
    "print(len(lon_step_dict))\n",
    "print(lat_step_dict)\n",
    "Height_map = np.zeros(shape = (len(lon_step),len(lat_step)))\n",
    "for data in height_data:\n",
    "    lat_index = lat_step_dict[round(data['latitude'],1)]\n",
    "    lon_index = lon_step_dict[round(data['longitude'],1)]\n",
    "    if not Height_map[lon_index][lat_index] == 0:\n",
    "        Height_map[lon_index][lat_index] = (\n",
    "            Height_map[lon_index][lat_index] + data['elevation'])\n",
    "    else:\n",
    "        Height_map[lon_index][lat_index] = data['elevation']\n",
    "#Read real PM25 data\n",
    "with open(\"PM25_Realdata.pkl\",\"rb\") as file:\n",
    "    PM25_real_data = pk.load(file)\n",
    "#Y transform to [date][lon][lat]\n",
    "date_count = (date(2017,7,1) - date(2017,3,1)).days\n",
    "Y = np.reshape(Y, (24 * date_count,len(lon_step),len(lat_step),1))\n",
    "#Use the lon_step and lat_step to determin whether there is a real data\n",
    "for longitude in lon_step:#Go through all the longitude in the lon_step\n",
    "    for latitude in lat_step:#Go throungh all the latitude in the lat_step\n",
    "        try: #Try the first element\n",
    "            PM25_real_data[longitude][latitude][0]\n",
    "            for i in range(24 * date_count):\n",
    "                if not PM25_real_data[longitude][latitude][i] == 0:\n",
    "                    Y[i][lon_step_dict[longitude]][lat_step_dict[latitude]] = PM25_real_data[longitude][latitude][i]\n",
    "        except:\n",
    "            #print(\"No (\",longitude,\",\",latitude,\") exists\")\n",
    "            pass\n",
    "Height_map = np.expand_dims(Height_map,axis = 0)\n",
    "Height_map = np.repeat(Height_map,24*date_count,axis = 0)\n",
    "Height_map = np.expand_dims(Height_map,axis = 3)\n",
    "#Get all the other feature\n",
    "temp = np.reshape(X[:,5],Y.shape)\n",
    "humidity = np.reshape(X[:,6],Y.shape)\n",
    "pressure = np.reshape(X[:,7],Y.shape)\n",
    "wind_speed = np.reshape(X[:,9],Y.shape)\n",
    "#Normalize all the data\n",
    "Y[:] = normalize(Y[:])\n",
    "temp[:] = normalize(temp[:])\n",
    "humidity[:] = normalize(humidity[:])\n",
    "pressure[:] = normalize(pressure[:])\n",
    "wind_speed[:] = normalize(wind_speed[:])\n",
    "Height_map[:] = normalize(Height_map[:])\n",
    "Y = np.concatenate((Y,temp,humidity,pressure,wind_speed,Height_map),axis = 3)\n",
    "print(Y.shape)\n",
    "#Get the longitude of the target site\n",
    "target_lon = lon_step_dict[116.3]\n",
    "#Get the latitude of the target site\n",
    "target_lat = lat_step_dict[39.9]\n",
    "#There  are 256 objects in pickle file and there are 10 pickle files\n",
    "validation_size = len(Y) - 2560\n",
    "#The label of validation batch\n",
    "Y_shift = np.zeros((validation_size - 48,48))\n",
    "for i in range(validation_size - 48):\n",
    "    Y_shift[i] = np.roll(Y[-validation_size:,target_lon,target_lat,0],i)[:48]\n",
    "print(Y_shift.shape)\n",
    "#Dump validation data\n",
    "with open(\"1hour_pm25_validation_array.pkl\",\"wb\") as file:\n",
    "    pk.dump((Y[-validation_size: -48],Y_shift),file)\n",
    "#Dump testing data\n",
    "#Assume that batch size is 256\n",
    "batch_size = 256\n",
    "for index in range(1, int(len(Y) / batch_size)):\n",
    "    with open(\"batch_number_\" + str(index) + \".pkl\",\"wb\") as file:\n",
    "        if (batch_size * index + 48) > len(Y):\n",
    "            end_batch = len(Y)\n",
    "        else:\n",
    "            end_batch = batch_size * index + 48\n",
    "        Y_shift_training = Y[batch_size * (index - 1):end_batch,target_lon,target_lat,0]\n",
    "        Y_shift_testing = np.zeros((batch_size,48))\n",
    "        for index_for_shifting in range(batch_size):\n",
    "             Y_shift_testing[index_for_shifting] = np.roll(Y_shift_training,\n",
    "                                                           index_for_shifting)[:48]\n",
    "        print(Y_shift_testing.shape)\n",
    "        pk.dump((Y[256 * (index - 1):256 * index],#Testing batch\n",
    "                 Y_shift_testing),file)#Testing label\n",
    "print(\"ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start building network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Validation Shape\n",
      "(320, 22, 8, 6)\n",
      "(320, 48)\n",
      "================\n",
      "Testing Shape\n",
      "(10, 256, 22, 8, 6)\n",
      "(10, 256, 48)\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "#print(validation_size)\n",
    "batch = list()\n",
    "batch_label = list()\n",
    "#Serialize all the batch\n",
    "for index in range(1,11):\n",
    "    with open(\"batch_number_\" + str(index) + \".pkl\",\"rb\") as file:\n",
    "        batch_read, batch_label_read = pk.load(file)\n",
    "        batch.append(batch_read)\n",
    "        batch_label.append(batch_label_read)\n",
    "#Serialize the area \n",
    "with open(\"lat_lon_range.pkl\",\"rb\") as file:\n",
    "    lat_step, lon_step = pk.load(file)\n",
    "\n",
    "with open(\"1hour_pm25_validation_array.pkl\",\"rb\") as file:\n",
    "    X_validation, Y_validation = pk.load(file)\n",
    "print(\"================\")\n",
    "print(\"Validation Shape\")\n",
    "print(X_validation.shape)\n",
    "print(Y_validation.shape)\n",
    "print(\"================\")\n",
    "#print(X_validation[-1])\n",
    "#print(Y_validation[-2])\n",
    "batch = np.array(batch)\n",
    "batch_label = np.array(batch_label)\n",
    "print(\"Testing Shape\")\n",
    "print(batch.shape)\n",
    "print(batch_label.shape)\n",
    "print(\"================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_layer(x_tensor, conv_num_outputs, conv_ksize, conv_strides):\n",
    "    x_shape = x_tensor.get_shape().as_list()\n",
    "    weights = tf.Variable(tf.truncated_normal(\n",
    "        [*conv_ksize[:],x_shape[-1],conv_num_outputs],\n",
    "        mean = 0, stddev = 0.1))\n",
    "    strides = [1,*conv_strides[:],1]\n",
    "    padding = \"SAME\"\n",
    "    convolution = tf.nn.conv2d(x_tensor,weights,strides,padding)\n",
    "    #No batch norm\n",
    "    #convolution = tf.layers.batch_normalization(convolution, training=training)\n",
    "    convolution = tf.nn.relu(convolution)\n",
    "    print(\"--------------\")\n",
    "    print(\"conv_2d\")\n",
    "    print(convolution.get_shape().as_list())\n",
    "    tf.summary.histogram(\"weights\",weights)\n",
    "    return convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Build_CNN(x):\n",
    "    with tf.name_scope(\"CNN_1\"):\n",
    "        CNN = tf.layers.conv2d(inputs = x,filters = 64,kernel_size = 3,padding = 'SAME',activation = tf.nn.relu)\n",
    "        CNN = tf.nn.relu(CNN)\n",
    "    with tf.name_scope(\"NIN\"):\n",
    "        CNN = tf.contrib.layers.fully_connected(CNN,32)\n",
    "        CNN = tf.nn.relu(CNN)\n",
    "        CNN = tf.nn.dropout(CNN,keep_prob=0.8)\n",
    "        #CNN = CNN_layer(x,64,(3,3),(1,1))\n",
    "    with tf.name_scope(\"CNN_2\"):\n",
    "        CNN = tf.layers.conv2d(inputs = x,filters = 64,kernel_size = 3,padding = 'SAME',activation = tf.nn.relu)\n",
    "        CNN = tf.nn.relu(CNN)\n",
    "        #CNN = CNN_layer(x,64,(3,3),(1,1))\n",
    "    with tf.name_scope(\"flatten\"):\n",
    "        CNN = tf.contrib.layers.flatten(CNN)\n",
    "    with tf.name_scope(\"FC1\"):\n",
    "        CNN = tf.contrib.layers.fully_connected(CNN,128)\n",
    "        CNN = tf.nn.relu(CNN)\n",
    "        CNN = tf.nn.dropout(CNN,keep_prob=0.8)\n",
    "    with tf.name_scope(\"FC2\"):\n",
    "        CNN = tf.contrib.layers.fully_connected(CNN,128)\n",
    "        CNN = tf.nn.relu(CNN)\n",
    "        CNN = tf.nn.dropout(CNN,keep_prob=0.8)\n",
    "    with tf.name_scope(\"output\"):\n",
    "        CNN = tf.contrib.layers.fully_connected(CNN,48)\n",
    "    return CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_loss(session,validation_batch,validation_label,loss):\n",
    "    loss = session.run(loss,feed_dict = {x:validation_batch,y:validation_label})\n",
    "    print(\"|loss:{:.7f}\".format(loss),\"|\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Reset the graph\n",
    "tf.reset_default_graph()\n",
    "#Input of the network\n",
    "x = tf.placeholder(tf.float32,shape = (None,len(lon_step),len(lat_step),6), name = 'x')\n",
    "y = tf.placeholder(tf.float32,shape = (None,48),name = 'y')\n",
    "#Build the network\n",
    "logits = Build_CNN(x)\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "#Define Loss and optimizer\n",
    "with tf.name_scope(\"loss\"):\n",
    "    a = tf.abs(tf.subtract(y,logits))\n",
    "    b = tf.add(y,logits)\n",
    "    loss = tf.scalar_mul(2, tf.reduce_mean(tf.divide(a, b)))\n",
    "    #loss = tf.losses.mean_squared_error(y,logits)\n",
    "tf.summary.scalar(\"loss\",loss)\n",
    "#MSE version \n",
    "#loss = tf.sqrt(tf.reduce_mean(tf.losses.mean_squared_error(labels = y,predictions = logits)))\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "merged = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|epoch: 0 |loss: 0.8033346 |\n",
      "|epoch: 30 |loss: 0.2463996 |\n",
      "|epoch: 60 |loss: 0.25768185 |\n",
      "|epoch: 90 |loss: 0.28462973 |\n",
      "|epoch: 120 |loss: 0.29952952 |\n",
      "|epoch: 150 |loss: 0.27610174 |\n",
      "|epoch: 180 |loss: 0.29062113 |\n",
      "|epoch: 210 |loss: 0.2804135 |\n",
      "|epoch: 240 |loss: 0.30636224 |\n",
      "|epoch: 270 |loss: 0.29725215 |\n",
      "|epoch: 300 |loss: 0.29709816 |\n",
      "|epoch: 330 |loss: 0.3035885 |\n",
      "|epoch: 360 |loss: 0.29938105 |\n",
      "|epoch: 390 |loss: 0.31408238 |\n",
      "|epoch: 420 |loss: 0.29536307 |\n",
      "|epoch: 450 |loss: 0.29119965 |\n",
      "|epoch: 480 |loss: 0.3081205 |\n",
      "|epoch: 510 |loss: 0.29846618 |\n",
      "|epoch: 540 |loss: 0.2944883 |\n",
      "|epoch: 570 |loss: 0.299892 |\n",
      "|epoch: 600 |loss: 0.29854217 |\n",
      "|epoch: 630 |loss: 0.2906981 |\n",
      "|epoch: 660 |loss: 0.28567043 |\n",
      "|epoch: 690 |loss: 0.29696238 |\n",
      "|epoch: 720 |loss: 0.2892268 |\n",
      "|epoch: 750 |loss: 0.2962274 |\n",
      "|epoch: 780 |loss: 0.29839453 |\n",
      "|epoch: 810 |loss: 0.29595774 |\n",
      "|epoch: 840 |loss: 0.29565436 |\n",
      "|epoch: 870 |loss: 0.3073035 |\n",
      "|epoch: 900 |loss: 0.30352077 |\n",
      "|epoch: 930 |loss: 0.29713014 |\n",
      "|epoch: 960 |loss: 0.29553556 |\n",
      "|epoch: 990 |loss: 0.2938572 |\n",
      "|epoch: 1020 |loss: 0.30978364 |\n",
      "|epoch: 1050 |loss: 0.2917251 |\n",
      "|epoch: 1080 |loss: 0.29843965 |\n",
      "|epoch: 1110 |loss: 0.29469106 |\n",
      "|epoch: 1140 |loss: 0.30892116 |\n",
      "|epoch: 1170 |loss: 0.30151597 |\n",
      "|epoch: 1200 |loss: 0.30123273 |\n",
      "|epoch: 1230 |loss: 0.2967031 |\n",
      "|epoch: 1260 |loss: 0.3041727 |\n",
      "|epoch: 1290 |loss: 0.3014923 |\n",
      "|epoch: 1320 |loss: 0.30321234 |\n",
      "|epoch: 1350 |loss: 0.30303162 |\n",
      "|epoch: 1380 |loss: 0.30081066 |\n",
      "|epoch: 1410 |loss: 0.29900044 |\n",
      "|epoch: 1440 |loss: 0.29269928 |\n",
      "|epoch: 1470 |loss: 0.30538374 |\n",
      "|epoch: 1500 |loss: 0.29617414 |\n",
      "|epoch: 1530 |loss: 0.3034641 |\n",
      "|epoch: 1560 |loss: 0.3087361 |\n",
      "|epoch: 1590 |loss: 0.28615344 |\n",
      "|epoch: 1620 |loss: 0.29649132 |\n",
      "|epoch: 1650 |loss: 0.2878361 |\n",
      "|epoch: 1680 |loss: 0.2979651 |\n",
      "|epoch: 1710 |loss: 0.3021965 |\n",
      "|epoch: 1740 |loss: 0.29544032 |\n",
      "|epoch: 1770 |loss: 0.29933637 |\n",
      "|epoch: 1800 |loss: 0.288352 |\n",
      "|epoch: 1830 |loss: 0.295626 |\n",
      "|epoch: 1860 |loss: 0.2969717 |\n",
      "|epoch: 1890 |loss: 0.2991632 |\n",
      "|epoch: 1920 |loss: 0.2932065 |\n",
      "|epoch: 1950 |loss: 0.30227667 |\n",
      "|epoch: 1980 |loss: 0.3020008 |\n",
      "|epoch: 2010 |loss: 0.29202417 |\n",
      "|epoch: 2040 |loss: 0.31584048 |\n",
      "|epoch: 2070 |loss: 0.3009524 |\n",
      "|epoch: 2100 |loss: 0.2929936 |\n",
      "|epoch: 2130 |loss: 0.29815537 |\n",
      "|epoch: 2160 |loss: 0.2894078 |\n",
      "|epoch: 2190 |loss: 0.2902974 |\n",
      "|epoch: 2220 |loss: 0.29758132 |\n",
      "|epoch: 2250 |loss: 0.28457487 |\n",
      "|epoch: 2280 |loss: 0.29906753 |\n",
      "|epoch: 2310 |loss: 0.2952375 |\n",
      "|epoch: 2340 |loss: 0.295636 |\n",
      "|epoch: 2370 |loss: 0.29689243 |\n",
      "|epoch: 2400 |loss: 0.2921862 |\n",
      "|epoch: 2430 |loss: 0.29102638 |\n",
      "|epoch: 2460 |loss: 0.29314065 |\n",
      "|epoch: 2490 |loss: 0.29351532 |\n",
      "|epoch: 2520 |loss: 0.30746794 |\n",
      "|epoch: 2550 |loss: 0.31090045 |\n",
      "|epoch: 2580 |loss: 0.30005348 |\n",
      "|epoch: 2610 |loss: 0.29920936 |\n",
      "|epoch: 2640 |loss: 0.3059128 |\n",
      "|epoch: 2670 |loss: 0.30510435 |\n",
      "|epoch: 2700 |loss: 0.29517245 |\n",
      "|epoch: 2730 |loss: 0.29270488 |\n",
      "|epoch: 2760 |loss: 0.29500768 |\n",
      "|epoch: 2790 |loss: 0.29941145 |\n",
      "|epoch: 2820 |loss: 0.29929036 |\n",
      "|epoch: 2850 |loss: 0.29947975 |\n",
      "|epoch: 2880 |loss: 0.28940582 |\n",
      "|epoch: 2910 |loss: 0.29471198 |\n",
      "|epoch: 2940 |loss: 0.29230005 |\n",
      "|epoch: 2970 |loss: 0.30180788 |\n",
      "|epoch: 3000 |loss: 0.29877672 |\n",
      "|epoch: 3030 |loss: 0.306944 |\n",
      "|epoch: 3060 |loss: 0.30096328 |\n",
      "|epoch: 3090 |loss: 0.2955792 |\n",
      "|epoch: 3120 |loss: 0.29558635 |\n",
      "|epoch: 3150 |loss: 0.30234414 |\n",
      "|epoch: 3180 |loss: 0.300893 |\n",
      "|epoch: 3210 |loss: 0.30112395 |\n",
      "|epoch: 3240 |loss: 0.30201188 |\n",
      "|epoch: 3270 |loss: 0.30247042 |\n",
      "|epoch: 3300 |loss: 0.29700744 |\n",
      "|epoch: 3330 |loss: 0.29592302 |\n",
      "|epoch: 3360 |loss: 0.3013685 |\n",
      "|epoch: 3390 |loss: 0.28906345 |\n",
      "|epoch: 3420 |loss: 0.2926536 |\n",
      "|epoch: 3450 |loss: 0.2955469 |\n",
      "|epoch: 3480 |loss: 0.29984662 |\n",
      "|epoch: 3510 |loss: 0.29255718 |\n",
      "|epoch: 3540 |loss: 0.3032271 |\n",
      "|epoch: 3570 |loss: 0.30591172 |\n",
      "|epoch: 3600 |loss: 0.2969449 |\n",
      "|epoch: 3630 |loss: 0.29809412 |\n",
      "|epoch: 3660 |loss: 0.29891005 |\n",
      "|epoch: 3690 |loss: 0.30316883 |\n",
      "|epoch: 3720 |loss: 0.31748044 |\n",
      "|epoch: 3750 |loss: 0.29566765 |\n",
      "|epoch: 3780 |loss: 0.29309645 |\n",
      "|epoch: 3810 |loss: 0.29753196 |\n",
      "|epoch: 3840 |loss: 0.29952693 |\n",
      "|epoch: 3870 |loss: 0.30125725 |\n",
      "|epoch: 3900 |loss: 0.30481023 |\n",
      "|epoch: 3930 |loss: 0.30611065 |\n",
      "|epoch: 3960 |loss: 0.30115554 |\n",
      "|epoch: 3990 |loss: 0.3089167 |\n",
      "|epoch: 4020 |loss: 0.31033102 |\n",
      "|epoch: 4050 |loss: 0.2989874 |\n",
      "|epoch: 4080 |loss: 0.30593002 |\n",
      "|epoch: 4110 |loss: 0.3022726 |\n",
      "|epoch: 4140 |loss: 0.3133612 |\n",
      "|epoch: 4170 |loss: 0.30031204 |\n",
      "|epoch: 4200 |loss: 0.30080262 |\n",
      "|epoch: 4230 |loss: 0.30682164 |\n",
      "|epoch: 4260 |loss: 0.30292514 |\n",
      "|epoch: 4290 |loss: 0.3015457 |\n",
      "|epoch: 4320 |loss: 0.3059727 |\n",
      "|epoch: 4350 |loss: 0.30639797 |\n",
      "|epoch: 4380 |loss: 0.30861962 |\n",
      "|epoch: 4410 |loss: 0.31046197 |\n",
      "|epoch: 4440 |loss: 0.3095829 |\n",
      "|epoch: 4470 |loss: 0.31443828 |\n",
      "|epoch: 4500 |loss: 0.31328878 |\n",
      "|epoch: 4530 |loss: 0.3002384 |\n",
      "|epoch: 4560 |loss: 0.30284315 |\n",
      "|epoch: 4590 |loss: 0.30801645 |\n",
      "|epoch: 4620 |loss: 0.30176026 |\n",
      "|epoch: 4650 |loss: 0.30386898 |\n",
      "|epoch: 4680 |loss: 0.3043279 |\n",
      "|epoch: 4710 |loss: 0.29760835 |\n",
      "|epoch: 4740 |loss: 0.30260983 |\n",
      "|epoch: 4770 |loss: 0.30987632 |\n",
      "|epoch: 4800 |loss: 0.3026985 |\n",
      "|epoch: 4830 |loss: 0.3034475 |\n",
      "|epoch: 4860 |loss: 0.3043678 |\n",
      "|epoch: 4890 |loss: 0.31038916 |\n",
      "|epoch: 4920 |loss: 0.28764403 |\n",
      "|epoch: 4950 |loss: 0.30337843 |\n",
      "|epoch: 4980 |loss: 0.2947562 |\n",
      "|epoch: 5010 |loss: 0.30205208 |\n",
      "|epoch: 5040 |loss: 0.30592138 |\n",
      "|epoch: 5070 |loss: 0.29817516 |\n",
      "|epoch: 5100 |loss: 0.29699486 |\n",
      "|epoch: 5130 |loss: 0.3030496 |\n",
      "|epoch: 5160 |loss: 0.2964056 |\n",
      "|epoch: 5190 |loss: 0.29969555 |\n",
      "|epoch: 5220 |loss: 0.30346715 |\n",
      "|epoch: 5250 |loss: 0.31192678 |\n",
      "|epoch: 5280 |loss: 0.30430466 |\n",
      "|epoch: 5310 |loss: 0.29367524 |\n",
      "|epoch: 5340 |loss: 0.3053548 |\n",
      "|epoch: 5370 |loss: 0.3029622 |\n",
      "|epoch: 5400 |loss: 0.30334234 |\n",
      "|epoch: 5430 |loss: 0.3054354 |\n",
      "|epoch: 5460 |loss: 0.2939497 |\n",
      "|epoch: 5490 |loss: 0.29590917 |\n",
      "|epoch: 5520 |loss: 0.29139793 |\n",
      "|epoch: 5550 |loss: 0.29452726 |\n",
      "|epoch: 5580 |loss: 0.29714012 |\n",
      "|epoch: 5610 |loss: 0.29653424 |\n",
      "|epoch: 5640 |loss: 0.30123642 |\n",
      "|epoch: 5670 |loss: 0.30791643 |\n",
      "|epoch: 5700 |loss: 0.30285335 |\n",
      "|epoch: 5730 |loss: 0.302724 |\n",
      "|epoch: 5760 |loss: 0.31218618 |\n",
      "|epoch: 5790 |loss: 0.30603385 |\n",
      "|epoch: 5820 |loss: 0.31059176 |\n",
      "|epoch: 5850 |loss: 0.30531186 |\n",
      "|epoch: 5880 |loss: 0.3099844 |\n",
      "|epoch: 5910 |loss: 0.3094557 |\n",
      "|epoch: 5940 |loss: 0.30478606 |\n",
      "|epoch: 5970 |loss: 0.30531698 |\n",
      "|epoch: 6000 |loss: 0.31381595 |\n",
      "|epoch: 6030 |loss: 0.30663514 |\n",
      "|epoch: 6060 |loss: 0.30811033 |\n",
      "|epoch: 6090 |loss: 0.303634 |\n",
      "|epoch: 6120 |loss: 0.30011874 |\n",
      "|epoch: 6150 |loss: 0.3104981 |\n",
      "|epoch: 6180 |loss: 0.30936202 |\n",
      "|epoch: 6210 |loss: 0.30775118 |\n",
      "|epoch: 6240 |loss: 0.30288804 |\n",
      "|epoch: 6270 |loss: 0.30482468 |\n",
      "|epoch: 6300 |loss: 0.31208494 |\n",
      "|epoch: 6330 |loss: 0.30788022 |\n",
      "|epoch: 6360 |loss: 0.2993963 |\n",
      "|epoch: 6390 |loss: 0.3099249 |\n",
      "|epoch: 6420 |loss: 0.3201048 |\n",
      "|epoch: 6450 |loss: 0.29972762 |\n",
      "|epoch: 6480 |loss: 0.30795395 |\n",
      "|epoch: 6510 |loss: 0.3088792 |\n",
      "|epoch: 6540 |loss: 0.29971308 |\n",
      "|epoch: 6570 |loss: 0.30259496 |\n",
      "|epoch: 6600 |loss: 0.31242043 |\n",
      "|epoch: 6630 |loss: 0.3136669 |\n",
      "|epoch: 6660 |loss: 0.30842942 |\n",
      "|epoch: 6690 |loss: 0.30533448 |\n",
      "|epoch: 6720 |loss: 0.29936907 |\n",
      "|epoch: 6750 |loss: 0.29720202 |\n",
      "|epoch: 6780 |loss: 0.30820265 |\n",
      "|epoch: 6810 |loss: 0.30896384 |\n",
      "|epoch: 6840 |loss: 0.31596863 |\n",
      "|epoch: 6870 |loss: 0.30483323 |\n",
      "|epoch: 6900 |loss: 0.30374777 |\n",
      "|epoch: 6930 |loss: 0.312358 |\n",
      "|epoch: 6960 |loss: 0.30669233 |\n",
      "|epoch: 6990 |loss: 0.31419033 |\n",
      "|epoch: 7020 |loss: 0.3112876 |\n",
      "|epoch: 7050 |loss: 0.29794008 |\n",
      "|epoch: 7080 |loss: 0.31456205 |\n",
      "|epoch: 7110 |loss: 0.32028225 |\n",
      "|epoch: 7140 |loss: 0.313175 |\n",
      "|epoch: 7170 |loss: 0.3093303 |\n",
      "|epoch: 7200 |loss: 0.3037177 |\n",
      "|epoch: 7230 |loss: 0.30744702 |\n",
      "|epoch: 7260 |loss: 0.30758506 |\n",
      "|epoch: 7290 |loss: 0.31147453 |\n",
      "|epoch: 7320 |loss: 0.30247518 |\n",
      "|epoch: 7350 |loss: 0.29962137 |\n",
      "|epoch: 7380 |loss: 0.29938915 |\n",
      "|epoch: 7410 |loss: 0.30934232 |\n",
      "|epoch: 7440 |loss: 0.2973163 |\n",
      "|epoch: 7470 |loss: 0.311598 |\n",
      "|epoch: 7500 |loss: 0.31046286 |\n",
      "|epoch: 7530 |loss: 0.30175704 |\n",
      "|epoch: 7560 |loss: 0.30768737 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|epoch: 7590 |loss: 0.32079464 |\n",
      "|epoch: 7620 |loss: 0.31806672 |\n",
      "|epoch: 7650 |loss: 0.31400132 |\n",
      "|epoch: 7680 |loss: 0.30586076 |\n",
      "|epoch: 7710 |loss: 0.30205792 |\n",
      "|epoch: 7740 |loss: 0.30490372 |\n",
      "|epoch: 7770 |loss: 0.3074006 |\n",
      "|epoch: 7800 |loss: 0.31043556 |\n",
      "|epoch: 7830 |loss: 0.31299278 |\n",
      "|epoch: 7860 |loss: 0.3117288 |\n",
      "|epoch: 7890 |loss: 0.30995256 |\n",
      "|epoch: 7920 |loss: 0.30223656 |\n",
      "|epoch: 7950 |loss: 0.3104766 |\n",
      "|epoch: 7980 |loss: 0.3045462 |\n",
      "|epoch: 8010 |loss: 0.3068568 |\n",
      "|epoch: 8040 |loss: 0.30882493 |\n",
      "|epoch: 8070 |loss: 0.3024131 |\n",
      "|epoch: 8100 |loss: 0.30175117 |\n",
      "|epoch: 8130 |loss: 0.30329114 |\n",
      "|epoch: 8160 |loss: 0.30622035 |\n",
      "|epoch: 8190 |loss: 0.30895475 |\n",
      "|epoch: 8220 |loss: 0.30158386 |\n",
      "|epoch: 8250 |loss: 0.30557042 |\n",
      "|epoch: 8280 |loss: 0.30106777 |\n",
      "|epoch: 8310 |loss: 0.30523258 |\n",
      "|epoch: 8340 |loss: 0.31643727 |\n",
      "|epoch: 8370 |loss: 0.3032422 |\n",
      "|epoch: 8400 |loss: 0.3119502 |\n",
      "|epoch: 8430 |loss: 0.31544352 |\n",
      "|epoch: 8460 |loss: 0.30927932 |\n",
      "|epoch: 8490 |loss: 0.30875957 |\n",
      "|epoch: 8520 |loss: 0.30828488 |\n",
      "|epoch: 8550 |loss: 0.3084164 |\n",
      "|epoch: 8580 |loss: 0.3092599 |\n",
      "|epoch: 8610 |loss: 0.3089612 |\n",
      "|epoch: 8640 |loss: 0.3086034 |\n",
      "|epoch: 8670 |loss: 0.30655047 |\n",
      "|epoch: 8700 |loss: 0.31408477 |\n",
      "|epoch: 8730 |loss: 0.3088725 |\n",
      "|epoch: 8760 |loss: 0.3132011 |\n",
      "|epoch: 8790 |loss: 0.31201154 |\n",
      "|epoch: 8820 |loss: 0.31429192 |\n",
      "|epoch: 8850 |loss: 0.3125126 |\n",
      "|epoch: 8880 |loss: 0.31546873 |\n",
      "|epoch: 8910 |loss: 0.31348145 |\n",
      "|epoch: 8940 |loss: 0.31642494 |\n",
      "|epoch: 8970 |loss: 0.32243598 |\n",
      "|epoch: 9000 |loss: 0.3049419 |\n",
      "|epoch: 9030 |loss: 0.3100006 |\n",
      "|epoch: 9060 |loss: 0.30142194 |\n",
      "|epoch: 9090 |loss: 0.3070742 |\n",
      "|epoch: 9120 |loss: 0.31117654 |\n",
      "|epoch: 9150 |loss: 0.3104519 |\n",
      "|epoch: 9180 |loss: 0.30799705 |\n",
      "|epoch: 9210 |loss: 0.3053552 |\n",
      "|epoch: 9240 |loss: 0.3086179 |\n",
      "|epoch: 9270 |loss: 0.3170503 |\n",
      "|epoch: 9300 |loss: 0.30872118 |\n",
      "|epoch: 9330 |loss: 0.30607563 |\n",
      "|epoch: 9360 |loss: 0.30699372 |\n",
      "|epoch: 9390 |loss: 0.31131724 |\n",
      "|epoch: 9420 |loss: 0.3053513 |\n",
      "|epoch: 9450 |loss: 0.31034654 |\n",
      "|epoch: 9480 |loss: 0.3064576 |\n",
      "|epoch: 9510 |loss: 0.3105501 |\n",
      "|epoch: 9540 |loss: 0.31459388 |\n",
      "|epoch: 9570 |loss: 0.3102139 |\n",
      "|epoch: 9600 |loss: 0.3113983 |\n",
      "|epoch: 9630 |loss: 0.30708122 |\n",
      "|epoch: 9660 |loss: 0.31370455 |\n",
      "|epoch: 9690 |loss: 0.30529165 |\n",
      "|epoch: 9720 |loss: 0.30567783 |\n",
      "|epoch: 9750 |loss: 0.30717617 |\n",
      "|epoch: 9780 |loss: 0.30190733 |\n",
      "|epoch: 9810 |loss: 0.30767757 |\n",
      "|epoch: 9840 |loss: 0.3170741 |\n",
      "|epoch: 9870 |loss: 0.32118338 |\n",
      "|epoch: 9900 |loss: 0.30055928 |\n",
      "|epoch: 9930 |loss: 0.30871847 |\n",
      "|epoch: 9960 |loss: 0.30813807 |\n",
      "|epoch: 9990 |loss: 0.30587763 |\n"
     ]
    }
   ],
   "source": [
    "epoch = 10000\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"log/pm25+temp\", graph = sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epoch):\n",
    "        for batch_num in range(0,10):\n",
    "            sess.run(optimizer,feed_dict = {x:batch[batch_num],y:batch_label[batch_num]})\n",
    "            #print(\"logis:\")\n",
    "            #print(logits.eval({x:X_validation}))\n",
    "            #print(\"Y:\")\n",
    "            #print(Y_validation)\n",
    "            loss_value = sess.run(loss,feed_dict = {x:X_validation,y:Y_validation})\n",
    "        if e % 30 == 0:\n",
    "            result = sess.run(merged,feed_dict = {x:X_validation,y:Y_validation})\n",
    "            writer.add_summary(result,e)\n",
    "            print(\"|epoch:\",e,\"|loss:\",loss_value,\"|\")\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,\"./PM25_predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
