{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/CN_AQ/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.461 116.146 116.30417\n",
      "40.09 39.824 39.935085\n"
     ]
    }
   ],
   "source": [
    "Urban_station_lonlats = np.array([\n",
    "['dongsi_aq',116.417,39.929],\n",
    "['tiantan_aq',116.407,39.886],\n",
    "['guanyuan_aq',116.339,39.929],\n",
    "['wanshouxigong_aq',116.352,39.878],\n",
    "['aotizhongxin_aq',116.397,39.982],\n",
    "['nongzhanguan_aq',116.461,39.937],\n",
    "['wanliu_aq',116.287,39.987],\n",
    "['beibuxinqu_aq',116.174,40.09],\n",
    "['zhiwuyuan_aq',116.207,40.002],\n",
    "['fengtaihuayuan_aq',116.279,39.863],\n",
    "['yungang_aq',116.146,39.824],\n",
    "['gucheng_aq',116.184,39.914]\n",
    "])\n",
    "lons = np.array(Urban_station_lonlats[:,1],dtype = np.float32)\n",
    "lats = np.array(Urban_station_lonlats[:,2],dtype = np.float32)\n",
    "print(lons.max(),lons.min(),lons.mean())\n",
    "print(lats.max(),lats.min(),lats.mean())\n",
    "X_length = 93.6\n",
    "Y_length = 41.2\n",
    "#Change X Y to degree\n",
    "Y_degree = Y_length / 111\n",
    "#Consider the length of the latitude line\n",
    "X_degree = X_length / (40000 * np.cos(lats.mean() / 180 * 3.1415926) / 360)\n",
    "#X_degree = X_length / 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39.6 39.7 39.8 39.9 40.  40.1 40.2 40.3]\n",
      "[115.2 115.3 115.4 115.5 115.6 115.7 115.8 115.9 116.  116.1 116.2 116.3\n",
      " 116.4 116.5 116.6 116.7 116.8 116.9 117.  117.1 117.2 117.3]\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "lat_step = np.arange(lats.mean() - Y_degree,lats.mean() + Y_degree,.1)\n",
    "lon_step = np.arange(lons.mean() - X_degree,lons.mean() + X_degree,.1)\n",
    "#Build the step map\n",
    "for index in range(len(lat_step)):\n",
    "    lat_step[index] = round(lat_step[index],1)\n",
    "for index in range(len(lon_step)):\n",
    "    lon_step[index] = round(lon_step[index],1)\n",
    "print(lat_step)\n",
    "print(lon_step)\n",
    "#Get the size of point waiting to be analyzed\n",
    "point_size = len(lat_step) * len(lon_step)\n",
    "with open(\"lat_lon_range.pkl\",\"wb\") as file:\n",
    "    pickle.dump((lat_step,lon_step),file)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        stationName  longitude  latitude             utc_time  temperature  \\\n",
      "0  beijing_grid_000      115.0      39.0  2017-01-01 00:00:00        -5.47   \n",
      "1  beijing_grid_001      115.0      39.1  2017-01-01 00:00:00        -5.53   \n",
      "2  beijing_grid_002      115.0      39.2  2017-01-01 00:00:00        -5.70   \n",
      "3  beijing_grid_003      115.0      39.3  2017-01-01 00:00:00        -5.88   \n",
      "4  beijing_grid_004      115.0      39.4  2017-01-01 00:00:00        -5.34   \n",
      "\n",
      "   pressure  humidity  wind_direction  wind_speed/kph  \n",
      "0    984.73     76.60           53.71            3.53  \n",
      "1    979.33     75.40           43.59            3.11  \n",
      "2    963.14     71.80            0.97            2.75  \n",
      "3    946.94     68.20          327.65            3.84  \n",
      "4    928.80     58.81          317.85            6.14  \n"
     ]
    }
   ],
   "source": [
    "#Use panda to read csv\n",
    "beijing_weather_data = pd.read_csv(\"Beijing_historical_meo_grid.csv\")\n",
    "print(beijing_weather_data.head())\n",
    "#Get the value of beijing data\n",
    "beijing_weather_data_value = beijing_weather_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from datetime import datetime\n",
    "#Build the datetime object\n",
    "for index in tqdm(range(len(beijing_weather_data_value))):\n",
    "    beijing_weather_data_value[index][3] = datetime.strptime(\n",
    "        beijing_weather_data_value[index][3],'%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Caculate the days between June 31 to March 1\n",
    "start = date(2017,3,1)\n",
    "end = date(2017,7,1)\n",
    "#Set the correct array size\n",
    "collection_size = point_size * (end - start).days * 24\n",
    "collection = [None] * collection_size\n",
    "#Select the data from March to June\n",
    "def get_Data(beijing_weather_data_value,collection,collection_size):\n",
    "    print(collection_size)\n",
    "    cursor = 0\n",
    "    for index in tqdm(range(beijing_weather_data_value.shape[0])):\n",
    "        for lat in lat_step:\n",
    "            for lon in lon_step:\n",
    "                if (beijing_weather_data_value[index][1] == lon and \n",
    "                    beijing_weather_data_value[index][2] == lat and\n",
    "                    beijing_weather_data_value[index][3].month >= 3 and\n",
    "                    beijing_weather_data_value[index][3].month <= 6):\n",
    "                    collection[cursor] = beijing_weather_data_value[index]\n",
    "                    cursor = cursor + 1\n",
    "                    if cursor == collection_size:\n",
    "                        print(cursor)\n",
    "                        return collection;\n",
    "#Extract all the data from CSV\n",
    "collection = get_Data(beijing_weather_data_value,collection,collection_size)\n",
    "with open(\"1hour.pkl\",\"wb\") as file:\n",
    "    pickle.dump(collection, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('ok!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beijing_grid_048' 115.2 39.6 datetime.datetime(2017, 3, 1, 0, 0) -4.4\n",
      " 916.13 35.47 333.37 22.56]\n",
      "{'_id': ObjectId('5ae019a950fc202dcd53e02f'), 'longitude': 115.29411764705883, 'latitude': 39.986486486486484, 'elevation': 1600.509399414062}\n"
     ]
    }
   ],
   "source": [
    "#Read the data\n",
    "import pickle\n",
    "collection = list()\n",
    "with open(\"1hour.pkl\",\"rb\") as file:\n",
    "    collection = pickle.load(file)\n",
    "print(collection[0])\n",
    "#Read height data\n",
    "with open(\"All_height_data_1hr.pk\",\"rb\") as file:\n",
    "    height_data = pickle.load(file)\n",
    "#print(round(height_data[-1]['latitude'],1))\n",
    "print(height_data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "22\n",
      "{39.6: 0, 39.7: 1, 39.8: 2, 39.9: 3, 40.0: 4, 40.1: 5, 40.2: 6, 40.3: 7}\n"
     ]
    }
   ],
   "source": [
    "lat_step_dict = {lat_step[i]:i for i in range(len(lat_step))}\n",
    "print(len(lat_step_dict))\n",
    "lon_step_dict = {lon_step[i]:i for i in range(len(lon_step))}\n",
    "print(len(lon_step_dict))\n",
    "print(lat_step_dict)\n",
    "Height_map = np.zeros(shape = (len(lon_step),len(lat_step)))\n",
    "for data in height_data:\n",
    "    lat_index = lat_step_dict[round(data['latitude'],1)]\n",
    "    lon_index = lon_step_dict[round(data['longitude'],1)]\n",
    "    if not Height_map[lon_index][lat_index] == 0:\n",
    "        Height_map[lon_index][lat_index] = (\n",
    "            Height_map[lon_index][lat_index] + data['elevation'])\n",
    "    else:\n",
    "        Height_map[lon_index][lat_index] = data['elevation']\n",
    "#print(Height_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 26849/515328 [00:00<00:01, 268378.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-03-01 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 515328/515328 [00:01<00:00, 315564.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(515328, 10)\n"
     ]
    }
   ],
   "source": [
    "#Prepare datas for SVR\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "print(collection[0][3])\n",
    "#collection[:,3] = datetime.strptime('%y-%m-%d %H:%M:%S',collection[:,3])\n",
    "lon = np.zeros(shape = (len(collection)))\n",
    "lat = np.zeros(shape = (len(collection)))\n",
    "month = np.zeros(shape = (len(collection)))\n",
    "date = np.zeros(shape = (len(collection)))\n",
    "hour = np.zeros(shape = (len(collection)))\n",
    "temp = np.zeros(shape = (len(collection)))\n",
    "humid = np.zeros(shape = (len(collection)))\n",
    "press = np.zeros(shape = (len(collection)))\n",
    "direct = np.zeros(shape = (len(collection)))\n",
    "speed = np.zeros(shape = (len(collection)))\n",
    "for index in tqdm(range(len(collection))):\n",
    "    lon[index] = collection[index][1]\n",
    "    lat[index] = collection[index][2]\n",
    "    month[index] = collection[index][3].month\n",
    "    date[index] = collection[index][3].day\n",
    "    hour[index] = collection[index][3].hour\n",
    "    temp[index] = collection[index][4]\n",
    "    humid[index] = collection[index][6]\n",
    "    press[index] = collection[index][5]\n",
    "    direct[index] = collection[index][7]\n",
    "    speed[index] = collection[index][8]\n",
    "lon = np.expand_dims(lon,axis = 1)\n",
    "lat = np.expand_dims(lat,axis = 1)\n",
    "month = np.expand_dims(month,axis = 1)\n",
    "date = np.expand_dims(date,axis = 1)\n",
    "hour = np.expand_dims(hour,axis = 1)\n",
    "temp = np.expand_dims(temp,axis = 1)\n",
    "humid = np.expand_dims(humid,axis = 1)\n",
    "press = np.expand_dims(press,axis = 1)\n",
    "direct = np.expand_dims(direct,axis = 1)\n",
    "speed = np.expand_dims(speed,axis = 1)\n",
    "X = np.concatenate((lon,lat,month,date,hour,temp,humid,press,direct,speed),axis = 1)\n",
    "with open(\"1hour_prepared.pkl\",\"wb\") as file:\n",
    "    pickle.dump(X, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[115.2   39.6    3.     1.     0.    -4.4   35.47 916.13 333.37  22.56]\n",
      " [115.2   39.7    3.     1.     0.    -5.79  39.06 905.54 332.07  25.16]\n",
      " [115.2   39.8    3.     1.     0.    -7.14  42.96 892.6  330.67  26.78]\n",
      " [115.2   39.9    3.     1.     0.    -8.44  47.44 874.98 328.6   26.46]\n",
      " [115.2   40.     3.     1.     0.    -9.73  51.93 857.35 326.49  26.17]\n",
      " [115.2   40.1    3.     1.     0.    -8.61  48.58 880.25 324.21  27.03]\n",
      " [115.2   40.2    3.     1.     0.    -7.48  45.24 903.14 322.07  27.92]\n",
      " [115.2   40.3    3.     1.     0.    -6.74  42.42 917.75 322.45  28.27]\n",
      " [115.3   39.6    3.     1.     0.    -2.54  32.41 939.33 337.13  17.22]\n",
      " [115.3   39.7    3.     1.     0.    -3.58  33.74 938.19 334.49  21.71]]\n",
      "(515328, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.經度(longt)\\n1.緯度(lat)\\n2.月份(month)\\n3.小時(hour)\\n4.溫度(temp)\\n5.濕度(humidity)\\n6.氣壓(pressure)\\n7.風向(wind_direction)\\n8.風速(wind_speed)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deserialize all the raw grid data\n",
    "import pickle\n",
    "with open(\"1hour_prepared.pkl\",\"rb\") as file:\n",
    "    X = pickle.load(file)\n",
    "print(X[:10])\n",
    "print(X.shape)\n",
    "'''0.經度(longt)\n",
    "1.緯度(lat)\n",
    "2.月份(month)\n",
    "3.小時(hour)\n",
    "4.溫度(temp)\n",
    "5.濕度(humidity)\n",
    "6.氣壓(pressure)\n",
    "7.風向(wind_direction)\n",
    "8.風速(wind_speed)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#SVR analysis\n",
    "'''X => SVR輸入向量的numpy array，資料必須照著下面的順序:\n",
    "0.經度(longt)\n",
    "1.緯度(lat)\n",
    "2.月份(month)\n",
    "3.天數(day)\n",
    "4.小時(hour)\n",
    "5.溫度(temp)\n",
    "6.濕度(humidity)\n",
    "7.氣壓(pressure)\n",
    "8.風向(wind_direction)\n",
    "9.風速(wind_speed)\n",
    "Y => PM2.5的預測值'''\n",
    "import pickle\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "\n",
    "svr = joblib.load('SVR_beijing.pkl')\n",
    "X = np.array(X, dtype=np.float64)\n",
    "X_mu = np.array([116.40616279,39.98908696,4.45972151,14.88423765,11.40249715,17.0021192,33.65642229,996.1066705,208.34196307,10.85087688])\n",
    "X_sigma = np.array([0.28161015,0.22815651,1.14854307,8.88446275,6.90495716,8.44671392,19.41044621,19.83802731,96.65489494,6.85870324])\n",
    "Y_mu = 59.541867360598516\n",
    "Y_sigma = 61.21714698899477\n",
    "\n",
    "Xs = (X - X_mu) / X_sigma #對要輸入的X向量標準化\n",
    "Y_pdt = svr.predict(Xs) #將標準化後的X向量放入SVR預測\n",
    "Y = Y_pdt * Y_sigma + Y_mu #對SVR得到的預測值反標準化\n",
    "print(Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "with open(\"1_hr_pm25.pkl\",\"wb\") as file:\n",
    "    pk.dump(Y, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize input data\n",
    "def normalize(x):\n",
    "    norm = (x - x.min())/(x.max() - x.min())\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2928, 22, 8, 1)\n",
      "(2928, 22, 8, 1)\n",
      "(2928, 22, 8, 6)\n",
      "ok!\n"
     ]
    }
   ],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "#load all the prepared data for SVR(to add feature to )\n",
    "with open(\"1hour_prepared.pkl\",\"rb\") as file:\n",
    "    X = pk.load(file)\n",
    "#load all the SVR data\n",
    "with open(\"1_hr_pm25.pkl\",\"rb\") as file:\n",
    "    Y = pk.load(file)\n",
    "#load the ercise point of latitude and longitude\n",
    "with open(\"lat_lon_range.pkl\",\"rb\") as file:\n",
    "    lat_step, lon_step = pk.load(file)\n",
    "#Y transform to [date][lon][lat]\n",
    "date_count = (date(2017,7,1) - date(2017,3,1)).days\n",
    "Y = np.reshape(Y, (24 * date_count,len(lon_step),len(lat_step),1))\n",
    "Y[:] = normalize(Y[:])\n",
    "Height_map = np.expand_dims(Height_map,axis = 0)\n",
    "Height_map = np.repeat(Height_map,24*date_count,axis = 0)\n",
    "Height_map = np.expand_dims(Height_map,axis = 3)\n",
    "print(Height_map.shape)\n",
    "#Get all the other feature\n",
    "temp = np.reshape(X[:,5],Y.shape)\n",
    "print(temp.shape)\n",
    "humidity = np.reshape(X[:,6],Y.shape)\n",
    "pressure = np.reshape(X[:,7],Y.shape)\n",
    "wind_speed = np.reshape(X[:,9],Y.shape)\n",
    "temp[:] = normalize(temp[:])\n",
    "humidity[:] = normalize(humidity[:])\n",
    "pressure[:] = normalize(pressure[:])\n",
    "wind_speed[:] = normalize(wind_speed[:])\n",
    "Height_map[:] = normalize(Height_map[:])\n",
    "Y = np.concatenate((Y,temp,humidity,pressure,wind_speed,Height_map),axis = 3)\n",
    "print(Y.shape)\n",
    "#Get the longitude of the target site\n",
    "target_lon = np.argwhere(lon_step == 116.3)\n",
    "#Get the latitude of the target site\n",
    "target_lat = np.argwhere(lat_step == 39.9)\n",
    "#There  are 256 objects in pickle file and there are 10 pickle files\n",
    "validation_size = len(Y) - 2560 \n",
    "#Dump validation data\n",
    "with open(\"1hour_pm25_validation_array.pkl\",\"wb\") as file:\n",
    "    pk.dump((Y[-validation_size - 1:-1],Y[-validation_size:,target_lon,target_lat,0]),file)\n",
    "#Dump testing data\n",
    "#Assume that batch size is 256\n",
    "for index in range(1, int(len(Y) / 256)):\n",
    "    with open(\"batch_number_\" + str(index) + \".pkl\",\"wb\") as file:\n",
    "        if 256 * index >= len(Y):\n",
    "            end_batch = len(Y) - 1\n",
    "        else:\n",
    "            end_batch = 256 * index\n",
    "        pk.dump((Y[256 * (index - 1):end_batch],#Testing batch\n",
    "                 Y[256 * (index - 1) + 1:end_batch + 1 ,target_lon,target_lat,0]),file)#Testing label\n",
    "print(\"ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start building network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Validation Shape\n",
      "(368, 22, 8, 6)\n",
      "(368, 1, 1)\n",
      "================\n",
      "Testing Shape\n",
      "(10, 256, 22, 8, 6)\n",
      "(10, 256, 1, 1)\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "#print(validation_size)\n",
    "batch = list()\n",
    "batch_label = list()\n",
    "#Serialize all the batch\n",
    "for index in range(1,11):\n",
    "    with open(\"batch_number_\" + str(index) + \".pkl\",\"rb\") as file:\n",
    "        batch_read, batch_label_read = pk.load(file)\n",
    "        batch.append(batch_read)\n",
    "        batch_label.append(batch_label_read)\n",
    "#Serialize the area \n",
    "with open(\"lat_lon_range.pkl\",\"rb\") as file:\n",
    "    lat_step, lon_step = pk.load(file)\n",
    "\n",
    "with open(\"1hour_pm25_validation_array.pkl\",\"rb\") as file:\n",
    "    X_validation, Y_validation = pk.load(file)\n",
    "print(\"================\")\n",
    "print(\"Validation Shape\")\n",
    "print(X_validation.shape)\n",
    "print(Y_validation.shape)\n",
    "print(\"================\")\n",
    "#print(X_validation[-1])\n",
    "#print(Y_validation[-2])\n",
    "batch = np.array(batch)\n",
    "batch_label = np.array(batch_label)\n",
    "print(\"Testing Shape\")\n",
    "print(batch.shape)\n",
    "print(batch_label.shape)\n",
    "print(\"================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_layer(x_tensor, conv_num_outputs, conv_ksize, conv_strides):\n",
    "    x_shape = x_tensor.get_shape().as_list()\n",
    "    weights = tf.Variable(tf.truncated_normal(\n",
    "        [*conv_ksize[:],x_shape[-1],conv_num_outputs],\n",
    "        mean = 0, stddev = 0.1))\n",
    "    strides = [1,*conv_strides[:],1]\n",
    "    padding = \"SAME\"\n",
    "    convolution = tf.nn.conv2d(x_tensor,weights,strides,padding)\n",
    "    #No batch norm\n",
    "    #convolution = tf.layers.batch_normalization(convolution, training=training)\n",
    "    convolution = tf.nn.relu(convolution)\n",
    "    print(\"--------------\")\n",
    "    print(\"conv_2d\")\n",
    "    print(convolution.get_shape().as_list())\n",
    "    tf.summary.histogram(\"weights\",weights)\n",
    "    return convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Build_CNN(x):\n",
    "    with tf.name_scope(\"CNN_1\"):\n",
    "        CNN = tf.layers.conv2d(inputs = x,filters = 64,kernel_size = 3,padding = 'SAME',activation = tf.nn.relu)\n",
    "        CNN = tf.nn.relu(CNN)\n",
    "        #CNN = CNN_layer(x,64,(3,3),(1,1))\n",
    "    with tf.name_scope(\"CNN_2\"):\n",
    "        CNN = tf.layers.conv2d(inputs = x,filters = 64,kernel_size = 3,padding = 'SAME',activation = tf.nn.relu)\n",
    "        CNN = tf.nn.relu(CNN)\n",
    "        #CNN = CNN_layer(x,64,(3,3),(1,1))\n",
    "    with tf.name_scope(\"flatten\"):\n",
    "        CNN = tf.contrib.layers.flatten(CNN)\n",
    "    with tf.name_scope(\"FC1\"):\n",
    "        CNN = tf.contrib.layers.fully_connected(CNN,128)\n",
    "        CNN = tf.nn.relu(CNN)\n",
    "    with tf.name_scope(\"FC2\"):\n",
    "        CNN = tf.contrib.layers.fully_connected(CNN,128)\n",
    "        CNN = tf.nn.relu(CNN)\n",
    "    with tf.name_scope(\"output\"):\n",
    "        CNN = tf.contrib.layers.fully_connected(CNN,1)\n",
    "    return CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_loss(session,validation_batch,validation_label,loss):\n",
    "    loss = session.run(loss,feed_dict = {x:validation_batch,y:validation_label})\n",
    "    print(\"|loss:{:.7f}\".format(loss),\"|\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Reset the graph\n",
    "tf.reset_default_graph()\n",
    "#Input of the network\n",
    "x = tf.placeholder(tf.float32,shape = (None,len(lon_step),len(lat_step),6), name = 'x')\n",
    "y = tf.placeholder(tf.float32,shape = (None),name = 'y')\n",
    "#Build the network\n",
    "logits = Build_CNN(x)\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "#Define Loss and optimizer\n",
    "with tf.name_scope(\"loss\"):\n",
    "    a = tf.abs(tf.subtract(y,logits))\n",
    "    b = tf.add(y,logits)\n",
    "    loss = tf.scalar_mul(2, tf.reduce_mean(tf.divide(a, b)))\n",
    "    #loss = tf.losses.mean_squared_error(y,logits)\n",
    "tf.summary.scalar(\"loss\",loss)\n",
    "#MSE version \n",
    "#loss = tf.sqrt(tf.reduce_mean(tf.losses.mean_squared_error(labels = y,predictions = logits)))\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "merged = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|epoch: 0 |loss: 0.54659116 |\n",
      "|epoch: 30 |loss: 0.19722651 |\n",
      "|epoch: 60 |loss: 0.18983807 |\n",
      "|epoch: 90 |loss: 0.1880096 |\n",
      "|epoch: 120 |loss: 0.19704884 |\n",
      "|epoch: 150 |loss: 0.19665824 |\n",
      "|epoch: 180 |loss: 0.19590442 |\n",
      "|epoch: 210 |loss: 0.19971833 |\n",
      "|epoch: 240 |loss: 0.18798965 |\n",
      "|epoch: 270 |loss: 0.19469532 |\n",
      "|epoch: 300 |loss: 0.18653558 |\n",
      "|epoch: 330 |loss: 0.18934838 |\n",
      "|epoch: 360 |loss: 0.1934515 |\n",
      "|epoch: 390 |loss: 0.18589643 |\n",
      "|epoch: 420 |loss: 0.1903142 |\n",
      "|epoch: 450 |loss: 0.19042705 |\n",
      "|epoch: 480 |loss: 0.18864582 |\n",
      "|epoch: 510 |loss: 0.18995896 |\n",
      "|epoch: 540 |loss: 0.18781576 |\n",
      "|epoch: 570 |loss: 0.18939564 |\n",
      "|epoch: 600 |loss: 0.19430941 |\n",
      "|epoch: 630 |loss: 0.19370644 |\n",
      "|epoch: 660 |loss: 0.19492406 |\n",
      "|epoch: 690 |loss: 0.19028418 |\n",
      "|epoch: 720 |loss: 0.18900067 |\n",
      "|epoch: 750 |loss: 0.19516858 |\n",
      "|epoch: 780 |loss: 0.19192082 |\n",
      "|epoch: 810 |loss: 0.19551618 |\n",
      "|epoch: 840 |loss: 0.19677356 |\n",
      "|epoch: 870 |loss: 0.20082249 |\n",
      "|epoch: 900 |loss: 0.19098744 |\n",
      "|epoch: 930 |loss: 0.19432862 |\n",
      "|epoch: 960 |loss: 0.1903797 |\n",
      "|epoch: 990 |loss: 0.19144078 |\n",
      "|epoch: 1020 |loss: 0.19568565 |\n",
      "|epoch: 1050 |loss: 0.19437943 |\n",
      "|epoch: 1080 |loss: 0.19063081 |\n",
      "|epoch: 1110 |loss: 0.19509663 |\n",
      "|epoch: 1140 |loss: 0.19683892 |\n",
      "|epoch: 1170 |loss: 0.18929744 |\n",
      "|epoch: 1200 |loss: 0.19197538 |\n",
      "|epoch: 1230 |loss: 0.19364148 |\n",
      "|epoch: 1260 |loss: 0.19595435 |\n",
      "|epoch: 1290 |loss: 0.19183566 |\n",
      "|epoch: 1320 |loss: 0.18900034 |\n",
      "|epoch: 1350 |loss: 0.19073492 |\n",
      "|epoch: 1380 |loss: 0.18904053 |\n",
      "|epoch: 1410 |loss: 0.18916056 |\n",
      "|epoch: 1440 |loss: 0.19673462 |\n",
      "|epoch: 1470 |loss: 0.19719483 |\n",
      "|epoch: 1500 |loss: 0.19792223 |\n",
      "|epoch: 1530 |loss: 0.1950621 |\n",
      "|epoch: 1560 |loss: 0.19498691 |\n",
      "|epoch: 1590 |loss: 0.19067739 |\n",
      "|epoch: 1620 |loss: 0.19484842 |\n",
      "|epoch: 1650 |loss: 0.19471446 |\n",
      "|epoch: 1680 |loss: 0.19002967 |\n",
      "|epoch: 1710 |loss: 0.19134818 |\n",
      "|epoch: 1740 |loss: 0.18959635 |\n",
      "|epoch: 1770 |loss: 0.18888214 |\n",
      "|epoch: 1800 |loss: 0.18860705 |\n",
      "|epoch: 1830 |loss: 0.18957748 |\n",
      "|epoch: 1860 |loss: 0.1897787 |\n",
      "|epoch: 1890 |loss: 0.1888961 |\n",
      "|epoch: 1920 |loss: 0.18932241 |\n",
      "|epoch: 1950 |loss: 0.18993852 |\n",
      "|epoch: 1980 |loss: 0.194002 |\n",
      "|epoch: 2010 |loss: 0.19517823 |\n",
      "|epoch: 2040 |loss: 0.19174641 |\n",
      "|epoch: 2070 |loss: 0.19208027 |\n",
      "|epoch: 2100 |loss: 0.1922177 |\n",
      "|epoch: 2130 |loss: 0.19136867 |\n",
      "|epoch: 2160 |loss: 0.19480571 |\n",
      "|epoch: 2190 |loss: 0.19173227 |\n",
      "|epoch: 2220 |loss: 0.1891634 |\n",
      "|epoch: 2250 |loss: 0.19207273 |\n",
      "|epoch: 2280 |loss: 0.19098186 |\n",
      "|epoch: 2310 |loss: 0.19036527 |\n",
      "|epoch: 2340 |loss: 0.19048032 |\n",
      "|epoch: 2370 |loss: 0.1902951 |\n",
      "|epoch: 2400 |loss: 0.18938662 |\n",
      "|epoch: 2430 |loss: 0.18969989 |\n",
      "|epoch: 2460 |loss: 0.19032812 |\n",
      "|epoch: 2490 |loss: 0.192701 |\n",
      "|epoch: 2520 |loss: 0.18935938 |\n",
      "|epoch: 2550 |loss: 0.1892045 |\n",
      "|epoch: 2580 |loss: 0.19155298 |\n",
      "|epoch: 2610 |loss: 0.19203515 |\n",
      "|epoch: 2640 |loss: 0.19134142 |\n",
      "|epoch: 2670 |loss: 0.19184628 |\n",
      "|epoch: 2700 |loss: 0.19221659 |\n",
      "|epoch: 2730 |loss: 0.19257246 |\n",
      "|epoch: 2760 |loss: 0.19139543 |\n",
      "|epoch: 2790 |loss: 0.19283964 |\n",
      "|epoch: 2820 |loss: 0.19711378 |\n",
      "|epoch: 2850 |loss: 0.18989736 |\n",
      "|epoch: 2880 |loss: 0.1908422 |\n",
      "|epoch: 2910 |loss: 0.19074447 |\n",
      "|epoch: 2940 |loss: 0.19014575 |\n",
      "|epoch: 2970 |loss: 0.18979824 |\n",
      "|epoch: 3000 |loss: 0.18981478 |\n",
      "|epoch: 3030 |loss: 0.18986185 |\n",
      "|epoch: 3060 |loss: 0.18955383 |\n",
      "|epoch: 3090 |loss: 0.18839309 |\n",
      "|epoch: 3120 |loss: 0.1886323 |\n",
      "|epoch: 3150 |loss: 0.18951702 |\n",
      "|epoch: 3180 |loss: 0.19065383 |\n",
      "|epoch: 3210 |loss: 0.19324386 |\n",
      "|epoch: 3240 |loss: 0.19304515 |\n",
      "|epoch: 3270 |loss: 0.19325349 |\n",
      "|epoch: 3300 |loss: 0.19342785 |\n",
      "|epoch: 3330 |loss: 0.1929379 |\n",
      "|epoch: 3360 |loss: 0.19338486 |\n",
      "|epoch: 3390 |loss: 0.19344431 |\n",
      "|epoch: 3420 |loss: 0.19283114 |\n",
      "|epoch: 3450 |loss: 0.19265628 |\n",
      "|epoch: 3480 |loss: 0.1927212 |\n",
      "|epoch: 3510 |loss: 0.19232523 |\n",
      "|epoch: 3540 |loss: 0.19140692 |\n",
      "|epoch: 3570 |loss: 0.19099574 |\n",
      "|epoch: 3600 |loss: 0.19157481 |\n",
      "|epoch: 3630 |loss: 0.18987803 |\n",
      "|epoch: 3660 |loss: 0.1905423 |\n",
      "|epoch: 3690 |loss: 0.19140048 |\n",
      "|epoch: 3720 |loss: 0.19136882 |\n",
      "|epoch: 3750 |loss: 0.19158918 |\n",
      "|epoch: 3780 |loss: 0.19160058 |\n",
      "|epoch: 3810 |loss: 0.19132611 |\n",
      "|epoch: 3840 |loss: 0.19212922 |\n",
      "|epoch: 3870 |loss: 0.1917131 |\n",
      "|epoch: 3900 |loss: 0.19191821 |\n",
      "|epoch: 3930 |loss: 0.19057766 |\n",
      "|epoch: 3960 |loss: 0.1908223 |\n",
      "|epoch: 3990 |loss: 0.19018658 |\n",
      "|epoch: 4020 |loss: 0.18954475 |\n",
      "|epoch: 4050 |loss: 0.18937603 |\n",
      "|epoch: 4080 |loss: 0.18926702 |\n",
      "|epoch: 4110 |loss: 0.18915626 |\n",
      "|epoch: 4140 |loss: 0.18858534 |\n",
      "|epoch: 4170 |loss: 0.18908678 |\n",
      "|epoch: 4200 |loss: 0.19165988 |\n",
      "|epoch: 4230 |loss: 0.19192556 |\n",
      "|epoch: 4260 |loss: 0.19117132 |\n",
      "|epoch: 4290 |loss: 0.19068508 |\n",
      "|epoch: 4320 |loss: 0.19278774 |\n",
      "|epoch: 4350 |loss: 0.19004022 |\n",
      "|epoch: 4380 |loss: 0.19005668 |\n",
      "|epoch: 4410 |loss: 0.19056731 |\n",
      "|epoch: 4440 |loss: 0.19200076 |\n",
      "|epoch: 4470 |loss: 0.19242072 |\n",
      "|epoch: 4500 |loss: 0.19241267 |\n",
      "|epoch: 4530 |loss: 0.19341451 |\n",
      "|epoch: 4560 |loss: 0.19404304 |\n",
      "|epoch: 4590 |loss: 0.19150348 |\n",
      "|epoch: 4620 |loss: 0.1906825 |\n",
      "|epoch: 4650 |loss: 0.19030511 |\n",
      "|epoch: 4680 |loss: 0.19021548 |\n",
      "|epoch: 4710 |loss: 0.1900866 |\n",
      "|epoch: 4740 |loss: 0.1899229 |\n",
      "|epoch: 4770 |loss: 0.18985033 |\n",
      "|epoch: 4800 |loss: 0.18953922 |\n",
      "|epoch: 4830 |loss: 0.18991789 |\n",
      "|epoch: 4860 |loss: 0.18989682 |\n",
      "|epoch: 4890 |loss: 0.19047275 |\n",
      "|epoch: 4920 |loss: 0.1921818 |\n",
      "|epoch: 4950 |loss: 0.191582 |\n",
      "|epoch: 4980 |loss: 0.19125153 |\n",
      "|epoch: 5010 |loss: 0.18942766 |\n",
      "|epoch: 5040 |loss: 0.18956393 |\n",
      "|epoch: 5070 |loss: 0.18980695 |\n",
      "|epoch: 5100 |loss: 0.19012462 |\n",
      "|epoch: 5130 |loss: 0.18995568 |\n",
      "|epoch: 5160 |loss: 0.19028395 |\n",
      "|epoch: 5190 |loss: 0.1899187 |\n",
      "|epoch: 5220 |loss: 0.19052991 |\n",
      "|epoch: 5250 |loss: 0.19030729 |\n",
      "|epoch: 5280 |loss: 0.19068399 |\n",
      "|epoch: 5310 |loss: 0.19159 |\n",
      "|epoch: 5340 |loss: 0.19199885 |\n",
      "|epoch: 5370 |loss: 0.19188374 |\n",
      "|epoch: 5400 |loss: 0.19177377 |\n",
      "|epoch: 5430 |loss: 0.19214205 |\n",
      "|epoch: 5460 |loss: 0.19236082 |\n",
      "|epoch: 5490 |loss: 0.19222905 |\n",
      "|epoch: 5520 |loss: 0.1922309 |\n",
      "|epoch: 5550 |loss: 0.19289549 |\n",
      "|epoch: 5580 |loss: 0.19207466 |\n",
      "|epoch: 5610 |loss: 0.19354105 |\n",
      "|epoch: 5640 |loss: 0.19119841 |\n",
      "|epoch: 5670 |loss: 0.19059193 |\n",
      "|epoch: 5700 |loss: 0.19102278 |\n",
      "|epoch: 5730 |loss: 0.1908618 |\n",
      "|epoch: 5760 |loss: 0.19078599 |\n",
      "|epoch: 5790 |loss: 0.1905212 |\n",
      "|epoch: 5820 |loss: 0.19064906 |\n",
      "|epoch: 5850 |loss: 0.19052342 |\n",
      "|epoch: 5880 |loss: 0.19062084 |\n",
      "|epoch: 5910 |loss: 0.19016622 |\n",
      "|epoch: 5940 |loss: 0.18996218 |\n",
      "|epoch: 5970 |loss: 0.19061181 |\n",
      "|epoch: 6000 |loss: 0.19096336 |\n",
      "|epoch: 6030 |loss: 0.19059803 |\n",
      "|epoch: 6060 |loss: 0.19106859 |\n",
      "|epoch: 6090 |loss: 0.19116516 |\n",
      "|epoch: 6120 |loss: 0.19118789 |\n",
      "|epoch: 6150 |loss: 0.19127986 |\n",
      "|epoch: 6180 |loss: 0.19122714 |\n",
      "|epoch: 6210 |loss: 0.19106635 |\n",
      "|epoch: 6240 |loss: 0.19116604 |\n",
      "|epoch: 6270 |loss: 0.19151624 |\n",
      "|epoch: 6300 |loss: 0.19199742 |\n",
      "|epoch: 6330 |loss: 0.19156322 |\n",
      "|epoch: 6360 |loss: 0.19144209 |\n",
      "|epoch: 6390 |loss: 0.19152486 |\n",
      "|epoch: 6420 |loss: 0.18544407 |\n",
      "|epoch: 6450 |loss: 0.19170004 |\n",
      "|epoch: 6480 |loss: 0.19227466 |\n",
      "|epoch: 6510 |loss: 0.19231296 |\n",
      "|epoch: 6540 |loss: 0.19233713 |\n",
      "|epoch: 6570 |loss: 0.19233441 |\n",
      "|epoch: 6600 |loss: 0.19224903 |\n",
      "|epoch: 6630 |loss: 0.19194356 |\n",
      "|epoch: 6660 |loss: 0.19217598 |\n",
      "|epoch: 6690 |loss: 0.19170475 |\n",
      "|epoch: 6720 |loss: 0.19129775 |\n",
      "|epoch: 6750 |loss: 0.1905601 |\n",
      "|epoch: 6780 |loss: 0.19141155 |\n",
      "|epoch: 6810 |loss: 0.19026393 |\n",
      "|epoch: 6840 |loss: 0.190692 |\n",
      "|epoch: 6870 |loss: 0.19063444 |\n",
      "|epoch: 6900 |loss: 0.19007093 |\n",
      "|epoch: 6930 |loss: 0.19013111 |\n",
      "|epoch: 6960 |loss: 0.18984768 |\n",
      "|epoch: 6990 |loss: 0.19010866 |\n",
      "|epoch: 7020 |loss: 0.18976402 |\n",
      "|epoch: 7050 |loss: 0.18985549 |\n",
      "|epoch: 7080 |loss: 0.19014014 |\n",
      "|epoch: 7110 |loss: 0.19038753 |\n",
      "|epoch: 7140 |loss: 0.19096336 |\n",
      "|epoch: 7170 |loss: 0.19044144 |\n",
      "|epoch: 7200 |loss: 0.19038196 |\n",
      "|epoch: 7230 |loss: 0.1905213 |\n",
      "|epoch: 7260 |loss: 0.1903981 |\n",
      "|epoch: 7290 |loss: 0.19054124 |\n",
      "|epoch: 7320 |loss: 0.18985477 |\n",
      "|epoch: 7350 |loss: 0.18979955 |\n",
      "|epoch: 7380 |loss: 0.18967453 |\n",
      "|epoch: 7410 |loss: 0.19065903 |\n",
      "|epoch: 7440 |loss: 0.18938805 |\n",
      "|epoch: 7470 |loss: 0.19073446 |\n",
      "|epoch: 7500 |loss: 0.19028318 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|epoch: 7530 |loss: 0.18973465 |\n",
      "|epoch: 7560 |loss: 0.18899208 |\n",
      "|epoch: 7590 |loss: 0.19047569 |\n",
      "|epoch: 7620 |loss: 0.19021532 |\n",
      "|epoch: 7650 |loss: 0.19082408 |\n",
      "|epoch: 7680 |loss: 0.19000405 |\n",
      "|epoch: 7710 |loss: 0.19006136 |\n",
      "|epoch: 7740 |loss: 0.19004521 |\n",
      "|epoch: 7770 |loss: 0.19006303 |\n",
      "|epoch: 7800 |loss: 0.19050159 |\n",
      "|epoch: 7830 |loss: 0.19066323 |\n",
      "|epoch: 7860 |loss: 0.19073258 |\n",
      "|epoch: 7890 |loss: 0.190693 |\n",
      "|epoch: 7920 |loss: 0.18979037 |\n",
      "|epoch: 7950 |loss: 0.18893199 |\n",
      "|epoch: 7980 |loss: 0.19075042 |\n",
      "|epoch: 8010 |loss: 0.19106726 |\n",
      "|epoch: 8040 |loss: 0.19066676 |\n",
      "|epoch: 8070 |loss: 0.19112007 |\n",
      "|epoch: 8100 |loss: 0.19105421 |\n",
      "|epoch: 8130 |loss: 0.19093134 |\n",
      "|epoch: 8160 |loss: 0.1909946 |\n",
      "|epoch: 8190 |loss: 0.19084622 |\n",
      "|epoch: 8220 |loss: 0.19098693 |\n",
      "|epoch: 8250 |loss: 0.19000036 |\n",
      "|epoch: 8280 |loss: 0.19078821 |\n",
      "|epoch: 8310 |loss: 0.1905069 |\n",
      "|epoch: 8340 |loss: 0.18956289 |\n",
      "|epoch: 8370 |loss: 0.19050853 |\n",
      "|epoch: 8400 |loss: 0.18970998 |\n",
      "|epoch: 8430 |loss: 0.19005673 |\n",
      "|epoch: 8460 |loss: 0.18976158 |\n",
      "|epoch: 8490 |loss: 0.18962926 |\n",
      "|epoch: 8520 |loss: 0.18921942 |\n",
      "|epoch: 8550 |loss: 0.18941069 |\n",
      "|epoch: 8580 |loss: 0.18866527 |\n",
      "|epoch: 8610 |loss: 0.18982273 |\n",
      "|epoch: 8640 |loss: 0.18914233 |\n",
      "|epoch: 8670 |loss: 0.18956926 |\n",
      "|epoch: 8700 |loss: 0.19034775 |\n",
      "|epoch: 8730 |loss: 0.189479 |\n",
      "|epoch: 8760 |loss: 0.19344164 |\n",
      "|epoch: 8790 |loss: 0.1905845 |\n",
      "|epoch: 8820 |loss: 0.19039612 |\n",
      "|epoch: 8850 |loss: 0.19029899 |\n",
      "|epoch: 8880 |loss: 0.18959214 |\n",
      "|epoch: 8910 |loss: 0.18959355 |\n",
      "|epoch: 8940 |loss: 0.19020385 |\n",
      "|epoch: 8970 |loss: 0.1904486 |\n",
      "|epoch: 9000 |loss: 0.1895132 |\n",
      "|epoch: 9030 |loss: 0.18994 |\n",
      "|epoch: 9060 |loss: 0.1896707 |\n",
      "|epoch: 9090 |loss: 0.19078255 |\n",
      "|epoch: 9120 |loss: 0.18992811 |\n",
      "|epoch: 9150 |loss: 0.19036318 |\n",
      "|epoch: 9180 |loss: 0.18882911 |\n",
      "|epoch: 9210 |loss: 0.19015537 |\n",
      "|epoch: 9240 |loss: 0.18998477 |\n",
      "|epoch: 9270 |loss: 0.18952571 |\n",
      "|epoch: 9300 |loss: 0.18744679 |\n",
      "|epoch: 9330 |loss: 0.18989377 |\n",
      "|epoch: 9360 |loss: 0.19069642 |\n",
      "|epoch: 9390 |loss: 0.19029692 |\n",
      "|epoch: 9420 |loss: 0.1903989 |\n",
      "|epoch: 9450 |loss: 0.19049518 |\n",
      "|epoch: 9480 |loss: 0.19021884 |\n",
      "|epoch: 9510 |loss: 0.18970317 |\n",
      "|epoch: 9540 |loss: 0.18999751 |\n",
      "|epoch: 9570 |loss: 0.19086185 |\n",
      "|epoch: 9600 |loss: 0.19038309 |\n",
      "|epoch: 9630 |loss: 0.19091956 |\n",
      "|epoch: 9660 |loss: 0.19012277 |\n",
      "|epoch: 9690 |loss: 0.19052401 |\n",
      "|epoch: 9720 |loss: 0.18945798 |\n",
      "|epoch: 9750 |loss: 0.18709446 |\n",
      "|epoch: 9780 |loss: 0.1895859 |\n",
      "|epoch: 9810 |loss: 0.18994248 |\n",
      "|epoch: 9840 |loss: 0.1892435 |\n",
      "|epoch: 9870 |loss: 0.1894459 |\n",
      "|epoch: 9900 |loss: 0.18924487 |\n",
      "|epoch: 9930 |loss: 0.1896608 |\n",
      "|epoch: 9960 |loss: 0.18971813 |\n",
      "|epoch: 9990 |loss: 0.19018504 |\n"
     ]
    }
   ],
   "source": [
    "epoch = 10000\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"log/pm25+temp\", graph = sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epoch):\n",
    "        for batch_num in range(0,10):\n",
    "            sess.run(optimizer,feed_dict = {x:batch[batch_num],y:batch_label[batch_num]})\n",
    "            #print(\"logis:\")\n",
    "            #print(logits.eval({x:X_validation}))\n",
    "            #print(\"Y:\")\n",
    "            #print(Y_validation)\n",
    "            loss_value = sess.run(loss,feed_dict = {x:X_validation,y:Y_validation})\n",
    "        if e % 30 == 0:\n",
    "            result = sess.run(merged,feed_dict = {x:X_validation,y:Y_validation})\n",
    "            writer.add_summary(result,e)\n",
    "            print(\"|epoch:\",e,\"|loss:\",loss_value,\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
